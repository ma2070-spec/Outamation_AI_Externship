{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZWI93O3Ri0/9ztMkJkTHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma2070-spec/Outamation_AI_Externship/blob/main/Task2_Completed_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV_K1o_1lHQ-"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U \\\n",
        "  llama-index llama-index-llms-gemini \\\n",
        "  llama-index-retrievers-bm25 llama-index-embeddings-huggingface \\\n",
        "  sentence-transformers pymupdf jedi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# set key once per runtime (hidden prompt)\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"ðŸ”‘ Paste your Gemini API key (hidden): \").strip()\n",
        "print(\"Key loaded âœ…\")\n",
        "\n",
        "# light, quota-friendly defaults\n",
        "Settings.llm = Gemini(model=\"gemini-1.5-flash\", temperature=0.15, max_tokens=256)\n",
        "Settings.embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "print(\"Settings ready âœ…\")\n"
      ],
      "metadata": {
        "id": "y-CBOa8Ml3bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDF = \"LenderFeesWorksheetNew.pdf\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(PDF):\n",
        "    from google.colab import files\n",
        "    print(\"Select the Lender Fee Worksheet PDFâ€¦\")\n",
        "    uploaded = files.upload()\n",
        "    PDF = list(uploaded.keys())[0]\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "docs = SimpleDirectoryReader(input_files=[PDF]).load_data()\n",
        "print(f\"Loaded {len(docs)} document(s)\")\n"
      ],
      "metadata": {
        "id": "Cx5Ihf6bl6ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Vector\n",
        "vector_index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
        "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=6)\n",
        "\n",
        "# Prepare nodes for BM25 variants\n",
        "try:\n",
        "    nodes = Settings.node_parser.get_nodes_from_documents(docs)\n",
        "except Exception:\n",
        "    nodes = None\n",
        "\n",
        "# BM25 import (old/new paths)\n",
        "try:\n",
        "    from llama_index.retrievers.bm25 import BM25Retriever\n",
        "except Exception:\n",
        "    from llama_index.core.retrievers import BM25Retriever\n",
        "\n",
        "bm25 = None; last_err = None\n",
        "# A) modern signature\n",
        "try:\n",
        "    bm25 = BM25Retriever.from_defaults(documents=docs, similarity_top_k=6)\n",
        "except Exception as e:\n",
        "    last_err = e\n",
        "# B) nodes signature\n",
        "if bm25 is None and nodes:\n",
        "    try:\n",
        "        bm25 = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=min(6, len(nodes)))\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "# C) docstore fallback\n",
        "if bm25 is None:\n",
        "    from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "    ds = SimpleDocumentStore()\n",
        "    if hasattr(ds, \"add_documents\"): ds.add_documents(docs)\n",
        "    elif hasattr(ds, \"add_nodes\") and nodes: ds.add_nodes(nodes)\n",
        "    bm25 = BM25Retriever.from_defaults(docstore=ds, similarity_top_k=6)\n",
        "\n",
        "# Clamp k so tiny PDFs donâ€™t crash BM25\n",
        "try:\n",
        "    corp_size = len(nodes) if nodes else 1\n",
        "    bm25.similarity_top_k = max(1, min(getattr(bm25, \"similarity_top_k\", 6), corp_size))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(f\"Vector@k={getattr(vector_retriever,'similarity_top_k',None)} | BM25@k={getattr(bm25,'similarity_top_k',None)} âœ…\")\n",
        "\n",
        "# Manual hybrid (works across all versions)\n",
        "try:\n",
        "    from llama_index.core.retrievers import BaseRetriever\n",
        "except Exception:\n",
        "    BaseRetriever = object\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "class ManualHybridRetriever(BaseRetriever):\n",
        "    def __init__(self, v, b, alpha=0.5, k=6):\n",
        "        self.v=v; self.b=b; self.alpha=float(alpha); self.k=int(k)\n",
        "    def _norm(self, items):\n",
        "        sc=[(i.score or 0.0) for i in items] or [0.0]\n",
        "        mn, mx = min(sc), max(sc); rng=(mx-mn) or 1.0\n",
        "        return {id(i.node):((i.score or 0.0)-mn)/rng for i in items}\n",
        "    async def _aretrieve(self, q): return self._retrieve(q)\n",
        "    def _retrieve(self, q):\n",
        "        vec=self.v.retrieve(q); kw=self.b.retrieve(q)\n",
        "        nv=self._norm(vec); nk=self._norm(kw)\n",
        "        merged={}\n",
        "        for lst in (vec,kw):\n",
        "            for nws in lst: merged.setdefault(id(nws.node), nws)\n",
        "        out=[]\n",
        "        for nid,nws in merged.items():\n",
        "            score=self.alpha*nv.get(nid,0.0)+(1-self.alpha)*nk.get(nid,0.0)\n",
        "            out.append(NodeWithScore(node=nws.node, score=score))\n",
        "        out.sort(key=lambda x: x.score or 0.0, reverse=True)\n",
        "        return out[:self.k]\n",
        "\n",
        "hybrid_retriever = ManualHybridRetriever(vector_retriever, bm25, alpha=0.5, k=6)\n",
        "print(\"Hybrid retriever ready âœ…\")\n"
      ],
      "metadata": {
        "id": "uUnJD5S5l65z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reranker = None\n",
        "try:\n",
        "    from llama_index.postprocessor import SentenceTransformerRerank\n",
        "    reranker = SentenceTransformerRerank(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3)\n",
        "except Exception:\n",
        "    try:\n",
        "        from llama_index.postprocessor.sentence_transformer_rerank import SentenceTransformerRerank\n",
        "        reranker = SentenceTransformerRerank(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3)\n",
        "    except Exception:\n",
        "        from sentence_transformers import CrossEncoder\n",
        "        class CEPostprocessor:\n",
        "            def __init__(self, model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3):\n",
        "                self.ce = CrossEncoder(model); self.top_n = top_n\n",
        "            def postprocess_nodes(self, nodes, query_bundle):\n",
        "                q = getattr(query_bundle,\"query_str\",str(query_bundle))\n",
        "                txts=[]\n",
        "                for n in nodes:\n",
        "                    node = getattr(n,\"node\",n)\n",
        "                    txts.append(node.get_text() if hasattr(node,\"get_text\") else str(getattr(n,\"text\",\"\")))\n",
        "                scores = self.ce.predict([[q,t] for t in txts])\n",
        "                for sc,n in zip(scores,nodes):\n",
        "                    try: n.score=float(sc)\n",
        "                    except: pass\n",
        "                ranked=[n for _,n in sorted(zip(scores,nodes), key=lambda t:t[0], reverse=True)]\n",
        "                return ranked[:self.top_n]\n",
        "        reranker = CEPostprocessor(top_n=3)\n",
        "print(\"Reranker ready âœ…\", type(reranker).__name__)\n"
      ],
      "metadata": {
        "id": "r3mLf8ixl7dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, time, textwrap\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core import Settings\n",
        "\n",
        "qe = RetrieverQueryEngine(retriever=hybrid_retriever, node_postprocessors=[reranker])\n",
        "\n",
        "def rewrite_query(q: str) -> str:\n",
        "    prompt = (\"Rewrite the user's question for precise retrieval in a lender fee worksheet. \"\n",
        "              \"Add key synonyms; keep it short.\\n\\n\"\n",
        "              f\"User: {q}\\nRewritten:\")\n",
        "    return Settings.llm.complete(prompt).text.strip()\n",
        "\n",
        "def ask_rag(q: str, expand=True, retries=3, base_sleep=1.2):\n",
        "    q2 = rewrite_query(q) if expand else q\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            resp = qe.query(q2)\n",
        "            print(f\"\\nQ: {q}\\nQ_expanded: {q2}\\n\")\n",
        "            print(\"Answer:\\n\", resp, \"\\n\")\n",
        "            print(\"Top sources:\")\n",
        "            for j, sn in enumerate(resp.source_nodes, 1):\n",
        "                node = getattr(sn,\"node\",sn)\n",
        "                name = getattr(node,\"metadata\",{}).get(\"file_name\",\"(unknown)\")\n",
        "                score = round((getattr(sn,\"score\",0) or 0), 3)\n",
        "                text = node.get_text() if hasattr(node,\"get_text\") else str(getattr(sn,\"text\",\"\"))\n",
        "                snippet = textwrap.shorten(text.replace(\"\\n\",\" \"), width=220)\n",
        "                print(f\"[{j}] score={score} file={name}\\n    {snippet}\")\n",
        "            return str(resp), resp.source_nodes\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e) or \"TooManyRequests\" in str(e):\n",
        "                sleep = base_sleep*(2**i)\n",
        "                print(f\"Rate limit: retrying in {sleep:.1f}sâ€¦\")\n",
        "                time.sleep(sleep)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "def extract_amount(sources, keywords):\n",
        "    \"\"\"Pick a $ amount near given keywords from top sources.\"\"\"\n",
        "    joined=\"\"\n",
        "    for sn in sources[:3]:\n",
        "        node = getattr(sn,\"node\",sn)\n",
        "        joined += \"\\n\" + (node.get_text() if hasattr(node,\"get_text\") else str(getattr(sn,\"text\",\"\")))\n",
        "    best=None\n",
        "    for line in joined.splitlines():\n",
        "        lo=line.lower()\n",
        "        if any(k in lo for k in keywords):\n",
        "            for m in re.findall(r\"\\$?\\s?\\d[\\d,]*(?:\\.\\d{2})?\", line):\n",
        "                best=m\n",
        "    if not best:\n",
        "        m=re.search(r\"\\$?\\s?\\d[\\d,]*(?:\\.\\d{2})?\", joined)\n",
        "        best=m.group(0) if m else None\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "gI2PpmhLmJxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 1\n",
        "ans1, src1 = ask_rag(\"What is the total estimated monthly payment?\")\n",
        "amt1 = extract_amount(src1, keywords=(\"monthly\",\"payment\",\"estimated\",\"total\"))\n",
        "print(\"\\nEstimated monthly payment (best evidence):\", amt1)\n",
        "\n",
        "# Prompt 2\n",
        "ans2, src2 = ask_rag(\"How much does the borrower pay for lender's title insurance?\")\n",
        "amt2 = extract_amount(src2, keywords=(\"lender\",\"title\",\"insurance\",\"premium\"))\n",
        "print(\"\\nLender's title insurance (best evidence):\", amt2)\n",
        "\n",
        "# Final deliverables text (copy-paste into your submission form)\n",
        "choices = (\n",
        "  \"- Embeddings: BAAI/bge-small-en-v1.5 â€” fast & strong for English retrieval.\\n\"\n",
        "  \"- Chunking: 1024 tokens, 200 overlap â€” balances recall with context.\\n\"\n",
        "  \"- Retrieval: Hybrid (BM25 + Vector, Î±=0.5) â€” exact fee names + semantic recall; cross-encoder reranker for precision.\"\n",
        ")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Short Explanation of Design Choices:\\n\" + choices)\n",
        "print(\"=\"*60)\n",
        "print(\"\\nResponse to Prompt 1:\\n\", ans1, f\"\\nEvidence amount: {amt1}\")\n",
        "print(\"\\nResponse to Prompt 2:\\n\", ans2, f\"\\nEvidence amount: {amt2}\")\n",
        "print(\"\\n(If an amount is None, the PDF likely doesnâ€™t list it explicitlyâ€”your answer text + sources still count.)\")\n"
      ],
      "metadata": {
        "id": "P3AI03WimKLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1: total estimated monthly payment\n",
        "ans1, src1 = ask_rag(\"What is the total estimated monthly payment?\", expand=True)\n",
        "\n",
        "print(\"\\n=== Paste into 'Short Explanation 1' ===\\n\")\n",
        "print(str(ans1))  # full AI-generated answer\n"
      ],
      "metadata": {
        "id": "-TYJb2oapQIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: lender's title insurance amount\n",
        "ans2, src2 = ask_rag(\"How much does the borrower pay for lender's title insurance?\", expand=True)\n",
        "\n",
        "print(\"\\n=== Paste into 'Short Explanation 2' ===\\n\")\n",
        "print(str(ans2))  # full AI-generated answer\n"
      ],
      "metadata": {
        "id": "wB81e_M1pVO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_amount(sources, keywords=()):\n",
        "    import re\n",
        "    text = \"\\n\".join(\n",
        "        (getattr(getattr(sn,'node',sn),'get_text',lambda:'' )() or str(getattr(sn,'text','')))\n",
        "        for sn in sources[:3]\n",
        "    )\n",
        "    for line in text.splitlines():\n",
        "        lo = line.lower()\n",
        "        if any(k in lo for k in keywords):\n",
        "            for m in re.findall(r\"\\$?\\s?\\d[\\d,]*(?:\\.\\d{2})?\", line):\n",
        "                return m\n",
        "    m = re.search(r\"\\$?\\s?\\d[\\d,]*(?:\\.\\d{2})?\", text)\n",
        "    return m.group(0) if m else \"Not specified\"\n",
        "\n",
        "print(\"Q1 amount:\", extract_amount(src1, (\"monthly\",\"payment\",\"estimated\",\"total\")))\n",
        "print(\"Q2 amount:\", extract_amount(src2, (\"lender\",\"title\",\"insurance\",\"premium\")))\n"
      ],
      "metadata": {
        "id": "kIOy5xYGpaoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "b1f6Ke9CuMIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) List any .ipynb files in your Drive (first 3 levels)\n",
        "!find \"/content/drive/MyDrive\" -maxdepth 3 -name \"*.ipynb\"\n"
      ],
      "metadata": {
        "id": "Izpj6WOXuZBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# List ipynb files under Drive (up to 3 levels deep)\n",
        "!find \"/content/drive/MyDrive\" -maxdepth 3 -name \"*.ipynb\"\n"
      ],
      "metadata": {
        "id": "wKYQK23bvs0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Make a GitHub-friendly copy of your Task II notebook ---\n",
        "\n",
        "import os, nbformat as nbf\n",
        "from google.colab import files\n",
        "\n",
        "# ðŸ‘‰ Choose ONE of your real notebook paths from the list you printed:\n",
        "SRC = \"/content/drive/MyDrive/Colab Notebooks/Task2_RAG_with_outputs.ipynb\"\n",
        "# (Alternatives you listed, if you prefer one of these instead:)\n",
        "# SRC = \"/content/drive/MyDrive/Colab Notebooks/Task II.Build & Optimize A RAG Pipeline For Doc Retrieval.ipynb\"\n",
        "# SRC = \"/content/drive/MyDrive/Colab Notebooks/Copy of Task II.Build & Optimize A RAG Pipeline For Doc Retrieval.ipynb\"\n",
        "\n",
        "assert os.path.exists(SRC), f\"Missing: {SRC}\"\n",
        "\n",
        "DST = \"/content/Task2_RAG_github.ipynb\"\n",
        "nb = nbf.read(SRC, as_version=4)\n",
        "\n",
        "# Strip ipywidgets metadata so GitHub stops saying \"Invalid Notebook\"\n",
        "for k in (\"widgets\",\"widget_state\",\"widgetsState\"):\n",
        "    nb.metadata.pop(k, None)\n",
        "\n",
        "KEEP_OUTPUTS = True   # set False if you want a smaller, clean notebook with no outputs\n",
        "if not KEEP_OUTPUTS:\n",
        "    for c in nb.cells:\n",
        "        if c.get(\"cell_type\") == \"code\":\n",
        "            c[\"outputs\"] = []\n",
        "            c[\"execution_count\"] = None\n",
        "\n",
        "nbf.write(nb, DST)\n",
        "print(\"Wrote:\", DST)\n",
        "\n",
        "# Download so you can upload it on GitHub\n",
        "files.download(DST)\n"
      ],
      "metadata": {
        "id": "5KW_pm9QwqoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive; drive.mount('/content/drive', force_remount=True)\n",
        "import os, nbformat as nbf\n",
        "from google.colab import files\n",
        "\n",
        "# Use your real path from Drive listing (you showed this exists):\n",
        "SRC = \"/content/drive/MyDrive/Colab Notebooks/Task2_RAG_with_outputs.ipynb\"\n",
        "# If you prefer a different one, swap:\n",
        "# SRC = \"/content/drive/MyDrive/Colab Notebooks/Task II.Build & Optimize A RAG Pipeline For Doc Retrieval.ipynb\"\n",
        "\n",
        "assert os.path.exists(SRC), f\"Missing: {SRC}\"\n",
        "\n",
        "DST = \"/content/Task2_RAG_github.ipynb\"\n",
        "nb = nbf.read(SRC, as_version=4)\n",
        "\n",
        "# --- Strip widget metadata at notebook root (if present)\n",
        "for k in (\"widgets\", \"widget_state\", \"widgetsState\"):\n",
        "    nb.metadata.pop(k, None)\n",
        "\n",
        "# --- Remove widget/HTML upload outputs from each cell but KEEP normal stdout text\n",
        "for cell in nb.cells:\n",
        "    if cell.get(\"cell_type\") == \"code\" and \"outputs\" in cell:\n",
        "        cleaned = []\n",
        "        for out in cell[\"outputs\"]:\n",
        "            # Keep plain text streams (your printed answers)\n",
        "            if out.get(\"output_type\") == \"stream\":\n",
        "                cleaned.append(out)\n",
        "                continue\n",
        "            # Drop widget views and the big upload widget HTML\n",
        "            data = out.get(\"data\", {})\n",
        "            if \"application/vnd.jupyter.widget-view+json\" in data:\n",
        "                continue\n",
        "            if \"text/html\" in data and \"Upload widget\" in \"\".join(data.get(\"text/html\") or []):\n",
        "                continue\n",
        "            cleaned.append(out)\n",
        "        cell[\"outputs\"] = cleaned\n",
        "\n",
        "    # Optional: remove cell-level widget references (cosmetic)\n",
        "    md = cell.get(\"metadata\", {})\n",
        "    colab_md = md.get(\"colab\", {})\n",
        "    colab_md.pop(\"referenced_widgets\", None)\n",
        "    md[\"colab\"] = colab_md\n",
        "    cell[\"metadata\"] = md\n",
        "\n",
        "nbf.write(nb, DST)\n",
        "print(\"Wrote cleaned notebook:\", DST)\n",
        "files.download(DST)\n"
      ],
      "metadata": {
        "id": "NraFlCyCyMJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}