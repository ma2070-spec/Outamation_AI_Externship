{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98398,
     "status": "ok",
     "timestamp": 1755445361952,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "PC9JMqUX3t3-",
    "outputId": "24e5d1bf-a1be-4f86-9833-c17be4421569"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U \\\n",
    "  llama-index \\\n",
    "  llama-index-llms-gemini \\\n",
    "  llama-index-retrievers-bm25 \\\n",
    "  llama-index-embeddings-huggingface \\\n",
    "  sentence-transformers \\\n",
    "  pymupdf \\\n",
    "  jedi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14990,
     "status": "ok",
     "timestamp": 1755445727624,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "OqX9orjX6klR",
    "outputId": "d2b0f320-3af0-4aaf-c3f6-5e59fe8c5b02"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = input(\"Paste Gemini API key: \").strip()\n",
    "print(\"Key loaded:\", bool(os.environ.get(\"GOOGLE_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 6765,
     "status": "ok",
     "timestamp": 1755445752988,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "8AA8R3SXuCC1",
    "outputId": "9219c8d5-acab-4cd0-e2b9-237162581dee"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "# Use the stable Gemini wrapper; no google-genai configure needed.\n",
    "_llm = Gemini(model=\"gemini-1.5-flash\")   # you can swap to \"gemini-1.5-pro\" later\n",
    "print(_llm.complete(\"Reply with the single word: ready\").text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545,
     "referenced_widgets": [
      "5892652b10b24bfcb2e906675cd3b65e",
      "ca662e979100466a8c7f5ca28f2f00fc",
      "138582b9c7ec43e09ca60aa84ac86909",
      "defab75cefbd418fb2a4dd6096473632",
      "4f74c3649e264a2588ad1313e3b6ee23",
      "3ca9ef1f16794ef58ccd1f8fa59fa3b6",
      "209bb60a835d4100b4542caa9be15f6f",
      "648d2123911742fe8b60a642ce765f95",
      "35fc08caf6a747fba616bac7cf658992",
      "9df0d5eda2fa41fc83a3fd7c1126462f",
      "797fa493f01244659e8e0e989d50e7c8",
      "51ae570df08145dba80fe770ccee56b9",
      "9e08c257d25c41a7bde38dc3096e3a48",
      "ddb01d84510c4a688b9a453fc84c7b1a",
      "8575f0ac4f6049e78653c19685dc4fbe",
      "c8e114949b45499da13dfa60f699fd0d",
      "18f2c5c20541435f9c06f8a6f9dea292",
      "ad64aeaeca144766aa2fdc6ae23a455e",
      "f0a3236860364f8883e619d7377dc657",
      "f8b4670f5e75472eb231a7f5837c587f",
      "ed6fa18c49aa430b9433852e08769da4",
      "2619b284f00c4a8e918cfd08b79782c6",
      "69e6f65834aa4532bcf3b82b2d4c7824",
      "af8845c8538049f1b02034699577cd4b",
      "17f66369ef8a48fca50519b54c851ca5",
      "ba9de4581a7f41b099d5faa6283feddf",
      "5f44ef7ebc824936896fab2859262675",
      "d09a1e9bf29c4436ad3332637e3f8377",
      "3834711c3cd14cf59d94f15de3760c9d",
      "92752d72ce6f45cd85ef1df5197ccce6",
      "dc05646f80cd4d0b9df0f5e326f95d19",
      "a9dcc18b6e5248c49276ee7923a8a651",
      "11345dfbf8474e90aa5f3933f0c607e0",
      "ee81810da9564151bcabac286e49152b",
      "e2573ef122e74a48ae620090774dc59b",
      "54dc99f80f5549748748faa87a24473a",
      "582c3998dd8c41589699b175a315f0ea",
      "51f674fcb6c74de6b017cc3773710338",
      "296ae6ed1b69490eb98f043650b07b8f",
      "92cb311e5bf3446ab761e3d2e2d90ec2",
      "b4db778651664eea8d95f12c7341b4e5",
      "4e34102710d344ec83d338614d62a14a",
      "d652dce2b0f34d579ca482d2fd1ab4b4",
      "60b34cc6a2244a4f8417227937f6a0f5",
      "c964e7e9010740f295173928fd0b3308",
      "5851638a7bb74ebc805dc80083449b7a",
      "39f413739a944a1f817d1757ee5479eb",
      "23a90fbd73364e8788af86cf0124d877",
      "f007e288bffb46e1b2d510c5e0dabe39",
      "446ef52f5f524ae283b4b4021be2a3fe",
      "25ef6e427dfa4e1f93e115a790067517",
      "2bb521087ac448acb701a3dec497a5fa",
      "0f675bf39b8045039951775495fe68cc",
      "a519b744c3f14c84ad1f46b6f2caa09c",
      "3cfbf737512646ef81657fac538938e1",
      "4f25c23a03aa4b5db8e7be1410d5e72a",
      "c9cbe22bb9e24816871fdd54306a305c",
      "de466c389e874e359486739e81e12470",
      "86166877e0a44d81803bc4f25e36ecb6",
      "0c303509020d4cd4aca8b24b3cc5ce66",
      "fe342d516f294e1dacd1602a3ab1c712",
      "473169722fdb4aa18cddd0a8f92d4999",
      "638c5eb72177417db045d7a4295e3d22",
      "9ca72475a60f4b78a158c1c7afed90d7",
      "9f3d16b77ea744fabf300ad0ff57efe4",
      "7895da39e3614158a700e588b9ea7080",
      "6cc7ccba790f4e5c835ae19f62d7c4c8",
      "6effd11600c745719be03c224b887de2",
      "228a4e7b87d64be0b9ce86e94878f580",
      "f35f9b4ee8a04ebf80a9b161aa8fc591",
      "a085cfa6ad4c4027937e89c83306f6c4",
      "bae86b93ecbf49d9b4ecbb64549e9436",
      "3bdb76088e80448cb965cb44bb5d49bc",
      "626e8b5edd754aa2bbf760e17cdcfddd",
      "be938905f1b9463ea5a641707d87cdec",
      "2784a158a18041f99a27b65edec0629f",
      "3dc86141979548f28c84cf2b94b44d9e",
      "16e5b5a450e14eae85305b04a6e2a14a",
      "5c1f404583a6440ea0ee5f99c40edfe7",
      "57936a3c83f7486994bbcf74f05f6487",
      "001c418b64ea474082cbbdadacd7ebf0",
      "80e19d3bc940454586a82dd73641ae89",
      "0806fb584da549d9a588c681427fe9c9",
      "f7cb8507952d4c82bf8aa18d2cd65052",
      "9d3823ec26b04d629a8d91571a5c2f1e",
      "6da3038681234d4a91ca5eeb6f5567aa",
      "db5fed8cebe34e1688dfd27535b3d90d",
      "d878a4f8f3514736ab81b8d16492ae85",
      "3f2c74c9ce3e43fdac7af0c932438108",
      "0c6e9f1f44c64233bf2e4fcd662f41c6",
      "357014acb88b427ca96c424396194424",
      "c09d7bb72d7142828793b1fb91411daf",
      "8ba326ee0b934bcab1a7dcd9e7116ccd",
      "659a496fb3f14a90b29953cdee897329",
      "85beec1dbb25430b95a48ae2a9ce4469",
      "c35202a690f0491b84a402f303c9ebce",
      "4311ba50494e47478261a782b93f0157",
      "0fdf846463cc4e80812d9a15f260a399",
      "8ce1c10d9b3c46f7924442644f51a40d",
      "4115b04725404bde8bd93b09917fe354",
      "4f292f3413484968b91ee974c1526bca",
      "bf18ee144006447e8015eb34a96ed1ff",
      "551bd99ec4ff4135a2f9b12d622f9ebf",
      "49eea805b8464ac99bdf8bbdd59838e0",
      "0789ac22c9ff4d729b1069a7c6ac0640",
      "1a32ef0e248646ef856f21fdd33b9e06",
      "7c0eb56cedab41e09bc38da3aa805c3c",
      "3a96cf8c9f434da4871cced49367a7dc",
      "162e8985c10f437f98dd9cfe16122272",
      "03fec65c9b3541c0be5e4b30c7a7f805",
      "7b36916349ed4c69a3079384703694cf",
      "a9903685613d47c88e8bd08666693d3f",
      "1b740c97b12c4e7c9a0ce5688542bd1c",
      "503ca7ee4ab34cedbaa9b29503fd5faf",
      "3ff3e2784e364d559d1e7b468dff1d9b",
      "06d4e7cd9b7f4d2896dd1abf79667d49",
      "b6fac9523dc642c2b76013a173da5b41",
      "ca6ccc488cfd4b669f8f3da14c366624",
      "d355c3dd9af7490784b50dbd9b11bbb3",
      "85c19ba001564f3fa6ddaa74e537df46",
      "ebfc016b03f64d4ca8c552891367876d"
     ]
    },
    "executionInfo": {
     "elapsed": 36297,
     "status": "ok",
     "timestamp": 1755445830915,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "Z4-XyMOjuMiL",
    "outputId": "140f99e5-8c89-48c8-831b-4faaf04b6da4"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "Settings.llm = Gemini(model=\"gemini-1.5-flash\", temperature=0.2, max_tokens=512)\n",
    "Settings.embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "\n",
    "print(\"Settings ready âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 15216,
     "status": "ok",
     "timestamp": 1755445978840,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "8CSGUE7nu4i-",
    "outputId": "6b995e0b-57f1-4ffd-8cd4-305c6cd37529"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()                   # pick 1+ PDFs\n",
    "pdf_paths = list(uploaded.keys())\n",
    "print(\"PDFs:\", pdf_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1755445996196,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "AJbPxz0du_9i",
    "outputId": "b4e153e6-9d2d-4ddb-c5f1-26bcf28a0ce0"
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def text_stats(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\\n\".join(page.get_text() for page in doc)\n",
    "    return len(text.split()), len(text)\n",
    "\n",
    "for p in pdf_paths:\n",
    "    w,c = text_stats(p)\n",
    "    print(f\"{p}: ~{w} words, {c} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "499b46114ea84c73873f459709803911",
      "0c9a32248ceb4aecafcc3de531fc9b70",
      "042a337c84594c8cb6a98d874bb42967",
      "c23556429e19473399b4a3339d8ad620",
      "85a486c8d2484021b2689d37fb8ed344",
      "cd24b9d170d8468da26d01a8c9da1e42",
      "745473d1278b45c79a55068c472a1856",
      "7be353162e8f4ab691ca002b7cdc93cb",
      "62aac2e2652d48e39b6185b101dbbef6",
      "b2137cb55e8945c4bf7e163e8eebad37",
      "aecab659146e4ff3845ba4209d2fe4cf",
      "312c55cbd91a40779d185c5de04f9e23",
      "cdd3d8ccdd80489f82a4c09da865c35d",
      "3f61c283bd0a4f4797bee58f2ea7ec55",
      "7bb04f66bec546189a31fe47577d18c7",
      "a71942f1a20749cf8865a7cc4475ac24",
      "939549f2cc134093accc685cb2e97603",
      "0f04c5a5f55440618844d31b03bc9737",
      "e07c745841a54b88aa2ba7f843fa967f",
      "9b6dacd474f9401a9dac05a3df4e9cab",
      "c71e6cf47cf24219b4e2c0b218479c79",
      "7a782dbfe1d04113a3c497e1195deb52"
     ]
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1755446552600,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "9XtfN9kfvD1_",
    "outputId": "ff38bd65-adea-402f-8946-64c204704b52"
   },
   "outputs": [],
   "source": [
    "# --- Step 5 (robust BM25 + Hybrid) ---\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SentenceSplitter  # if you didn't set Settings.node_parser\n",
    "\n",
    "# If you haven't loaded docs yet in this fresh runtime:\n",
    "# docs = SimpleDirectoryReader(input_files=pdf_paths).load_data()\n",
    "\n",
    "# Vector index + retriever\n",
    "vector_index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=6)\n",
    "\n",
    "# BM25 import (location differs by version)\n",
    "try:\n",
    "    from llama_index.retrievers.bm25 import BM25Retriever\n",
    "except Exception:\n",
    "    from llama_index.core.retrievers import BM25Retriever\n",
    "\n",
    "# Prepare nodes once (covers versions that require nodes=)\n",
    "try:\n",
    "    # Use your existing Settings.node_parser if you set one earlier\n",
    "    from llama_index.core import Settings\n",
    "    nodes = Settings.node_parser.get_nodes_from_documents(docs)\n",
    "except Exception:\n",
    "    # Fallback if no global parser set\n",
    "    splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "    nodes = splitter.get_nodes_from_documents(docs)\n",
    "\n",
    "bm25_retriever = None\n",
    "last_err = None\n",
    "\n",
    "# (A) Try the modern signature: documents=...\n",
    "try:\n",
    "    bm25_retriever = BM25Retriever.from_defaults(documents=docs, similarity_top_k=6)\n",
    "except Exception as e:\n",
    "    last_err = e\n",
    "\n",
    "# (B) Try nodes=... (older/alternate signature)\n",
    "if bm25_retriever is None:\n",
    "    try:\n",
    "        bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=6)\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "\n",
    "# (C) Docstore fallback (older APIs expect a docstore with add_documents)\n",
    "if bm25_retriever is None:\n",
    "    try:\n",
    "        from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "        ds = SimpleDocumentStore()\n",
    "        if hasattr(ds, \"add_documents\"):\n",
    "            ds.add_documents(docs)\n",
    "        elif hasattr(ds, \"add_nodes\"):\n",
    "            ds.add_nodes(nodes)\n",
    "        else:\n",
    "            raise RuntimeError(\"Docstore has neither add_documents nor add_nodes\")\n",
    "        bm25_retriever = BM25Retriever.from_defaults(docstore=ds, similarity_top_k=6)\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "\n",
    "if bm25_retriever is None:\n",
    "    raise RuntimeError(f\"Could not initialize BM25Retriever with this LlamaIndex version: {type(last_err).__name__}: {last_err}\")\n",
    "\n",
    "# Hybrid retriever (if available); otherwise weâ€™ll continue with vector-only\n",
    "try:\n",
    "    from llama_index.core.retrievers import HybridRetriever\n",
    "    hybrid_retriever = HybridRetriever(\n",
    "        vector_retriever=vector_retriever,\n",
    "        bm25_retriever=bm25_retriever,\n",
    "        alpha=0.5,  # 0=keyword-heavy, 1=semantic-heavy\n",
    "    )\n",
    "except Exception:\n",
    "    hybrid_retriever = None\n",
    "    print(\"HybridRetriever not found in this version â†’ using vector-only for now.\")\n",
    "\n",
    "print(\"BM25 + Hybrid setup âœ…\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1755447918817,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "wqsvvgQx2XLZ",
    "outputId": "a05e39f6-4ae1-4ea2-b147-501a3b6e9278"
   },
   "outputs": [],
   "source": [
    "# --- Manual Hybrid Retriever (Step 5.5) ---\n",
    "# Fuses vector + BM25 with an alpha weight and returns top-k nodes.\n",
    "\n",
    "try:\n",
    "    from llama_index.core.retrievers import BaseRetriever\n",
    "except Exception:\n",
    "    BaseRetriever = object  # fallback for older versions\n",
    "\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class ManualHybridRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever, alpha=0.5, k=6):\n",
    "        self.v = vector_retriever\n",
    "        self.b = bm25_retriever\n",
    "        self.alpha = float(alpha)\n",
    "        self.k = int(k)\n",
    "\n",
    "    def _normalize(self, items):\n",
    "        scores = [(i.score or 0.0) for i in items]\n",
    "        if not scores:\n",
    "            return {}\n",
    "        mn, mx = min(scores), max(scores)\n",
    "        rng = (mx - mn) or 1.0\n",
    "        return {id(i.node): ((i.score or 0.0) - mn)/rng for i in items}\n",
    "\n",
    "    async def _aretrieve(self, query_bundle):\n",
    "        return self._retrieve(query_bundle)\n",
    "\n",
    "    def _retrieve(self, query_bundle):\n",
    "        vec = self.v.retrieve(query_bundle)\n",
    "        kw  = self.b.retrieve(query_bundle)\n",
    "\n",
    "        nv = self._normalize(vec)\n",
    "        nk = self._normalize(kw)\n",
    "\n",
    "        merged = {}\n",
    "        for lst in (vec, kw):\n",
    "            for nws in lst:\n",
    "                merged.setdefault(id(nws.node), nws)\n",
    "\n",
    "        out = []\n",
    "        for nid, nws in merged.items():\n",
    "            score = self.alpha * nv.get(nid, 0.0) + (1 - self.alpha) * nk.get(nid, 0.0)\n",
    "            out.append(NodeWithScore(node=nws.node, score=score))\n",
    "\n",
    "        out.sort(key=lambda x: x.score or 0.0, reverse=True)\n",
    "        return out[: self.k]\n",
    "\n",
    "# Create a hybrid retriever instance so later steps see `hybrid_retriever`\n",
    "hybrid_retriever = ManualHybridRetriever(vector_retriever, bm25_retriever, alpha=0.5, k=6)\n",
    "print(\"Manual hybrid retriever ready âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "referenced_widgets": [
      "f1a5de96fb564970acec3333b683b127",
      "e627527c30de4acd85e5b8864fff2ab8",
      "6a735d5b50f945779b93ee455598ab67",
      "51a8a7924b3843c78af7cd64133c5dbc",
      "5878589f2eac4f0389f4112e7f442bde",
      "47811f91632348a19a7d8d474711fc5c",
      "a62995d1cfa14b898f34b611027463e7",
      "78078f662ac94896924568be4ac09f9c",
      "1aa7d8d633c24489812bf66cedb8edee",
      "4e87895072544b41a204a7495c6516dd",
      "ef2235dd572f4a8b869cb57c250212b1",
      "370f85a7d4ac456e8b079a1c5282da47",
      "7467b880e7284fe1a1f8394745bb27ad",
      "0fbab577140e42718603d6efcc75a052",
      "654ea190a0704b45b7c6339e963d402a",
      "b85ee3e4184f43aa822b0eadb174ca07",
      "d0294bf0b6a34b59aab9b23608a509d3",
      "7486057969444971901254a6597b728e",
      "11b12b9c5dc4488db4fbba554d76541b",
      "1aa02a57b0554793a9fab13012e5ab04",
      "91d0df6a16c346439743dcc33356bd2e",
      "88011411ef7c4ac59481fc304d836467",
      "11a6a1ada9d54286ba27c8675e2df587",
      "47d6dfd20d184beab8bcc34b5e11b966",
      "c4ad230d988f4344ab94989bd6312219",
      "d25d6d870534403a9a31a2b028717f57",
      "623f8f9ea0694eaeb975f0f08276fade",
      "bc0b45fe25e841198f76a9f5c68b4fe4",
      "ef872ba10e3c4359837b55c3821b43c5",
      "4fa04762d3b045b492535d34891f957b",
      "20c744e920b54baa94fd72f60486a56c",
      "8839388f81dc4831b86a8437f167e8a7",
      "cf4841462d3244b5a1f86881e044aea6",
      "db8ca29896e840488bb0322a66da2e09",
      "73145625ad584112b4d5d1631f4798ec",
      "bc7c283388454a31ba7843d8b7b9edef",
      "fa0d90d8ee6c424d98a3d1f3190e3bd2",
      "1d4ffcb9f8ce4f9d9a86075f029094f3",
      "782135cdf8cb4461b2aaf20746adbd05",
      "dcb5dfa77e464447a5b4b0b1c94f00ad",
      "0db76d8ee4af4c728036c68c354e2311",
      "22d4c2e42d5347dab145230009f5f382",
      "7f2a031976564813b141c46f268f35d1",
      "311f02f0f9064fa486355a10a06acc35",
      "67c0cb7b718c49cfb28cca6f628b9c26",
      "d3cebdaabc294b55a9d7adf4eb8cc63e",
      "c8a6af10c34c4343b076e5d5d0fbb32a",
      "f8a4179be4e549ab9dcf161c476cc59d",
      "2cd10eb9efa044089bd4f9ec6b48aae8",
      "9291b2a9cdbc4aeb8d0c75022c329dd3",
      "50423cbe073a42918d26400b429d20f5",
      "d909cff36f9949ca8eaa5318630b1883",
      "caebf7d6e07146d6b8984fa8035a9c9c",
      "5ef438be686b4b1ebab9b86d3001e4c0",
      "d2100a32b6f54f099f56414ca80ce50e",
      "ac4a3288d2af488abac882d1b2ac118d",
      "3608bec808274134ab58cc035e63597a",
      "82d29d19c66346b481ba30e276e203a9",
      "1cef31ce4a2c45b98a1740e9325eadd1",
      "f3185c02fab64cb3acf9e7fd96c82eae",
      "914743af82a54104b7c2e1f17da8455c",
      "e3913c5839364d89828ed82e74fa5071",
      "e4c3e9319a8848158cff29bf603fe3a1",
      "bb96358ac0994488ae0db6be2b8da088",
      "374bde424983413f93852a1ae1c0a4bb",
      "a595b843f7714d8e9ac3f16c229391f6",
      "e125206b0edd4c7aa333ef320f35d5f6",
      "95f8b274aed94c5c82ea4800c02b0bb9",
      "3d827213f86c4175abad2fc6227b96bd",
      "0da397fff49c4bf1b4c55f4d0ae78d00",
      "166c5ffd36594bca9943bf3fcd526d20",
      "7ea657d44a0c46d8b64b9756861ae91d",
      "88e0f24038674998b9afc2463569014c",
      "a20357d458084d7480ecb3fd611b1355",
      "c6b57d2f02f1435a81b788e2ac44b195",
      "d14a57fd6b5c42b5b977bb280c5abafd",
      "270e36bfd1d74c008889e5cff55b8e65"
     ]
    },
    "executionInfo": {
     "elapsed": 3896,
     "status": "ok",
     "timestamp": 1755446937612,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "Iit6lgN8ymsA",
    "outputId": "cd229dac-1755-4bb8-d85b-6ae2ca71e219"
   },
   "outputs": [],
   "source": [
    "# STEP 6 â€” Robust reranker (works even if llama_index.postprocessor is missing)\n",
    "\n",
    "# 0) Ensure deps (no-op if already installed)\n",
    "# If you didn't install these earlier, uncomment:\n",
    "# !pip -q install -U sentence-transformers\n",
    "\n",
    "# 1) Try LlamaIndex's built-in reranker first\n",
    "reranker = None\n",
    "try:\n",
    "    from llama_index.postprocessor import SentenceTransformerRerank  # some versions\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3\n",
    "    )\n",
    "except Exception:\n",
    "    try:\n",
    "        from llama_index.postprocessor.sentence_transformer_rerank import SentenceTransformerRerank  # other versions\n",
    "        reranker = SentenceTransformerRerank(\n",
    "            model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3\n",
    "        )\n",
    "    except Exception:\n",
    "        # 2) Fallback: custom reranker using sentence-transformers CrossEncoder\n",
    "        from sentence_transformers import CrossEncoder\n",
    "\n",
    "        class CEPostprocessor:\n",
    "            \"\"\"Minimal NodePostprocessor-compatible reranker.\"\"\"\n",
    "            def __init__(self, model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n: int = 3):\n",
    "                self.ce = CrossEncoder(model)\n",
    "                self.top_n = top_n\n",
    "\n",
    "            def postprocess_nodes(self, nodes, query_bundle):\n",
    "                # nodes can be NodeWithScore or TextNode depending on version\n",
    "                q = getattr(query_bundle, \"query_str\", str(query_bundle))\n",
    "                texts = []\n",
    "                for n in nodes:\n",
    "                    # NodeWithScore has .node.get_text(); TextNode has .get_text()\n",
    "                    if hasattr(n, \"node\") and hasattr(n.node, \"get_text\"):\n",
    "                        texts.append(n.node.get_text())\n",
    "                    elif hasattr(n, \"get_text\"):\n",
    "                        texts.append(n.get_text())\n",
    "                    else:\n",
    "                        # last-resort stringify\n",
    "                        texts.append(str(getattr(n, \"text\", n)))\n",
    "\n",
    "                pairs = [[q, t] for t in texts]\n",
    "                scores = self.ce.predict(pairs)  # higher = more relevant\n",
    "\n",
    "                # attach scores if possible, sort, and keep top_n\n",
    "                for sc, n in zip(scores, nodes):\n",
    "                    try:\n",
    "                        n.score = float(sc)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                nodes_sorted = [n for _, n in sorted(zip(scores, nodes), key=lambda t: t[0], reverse=True)]\n",
    "                return nodes_sorted[: self.top_n]\n",
    "\n",
    "        reranker = CEPostprocessor(top_n=3)\n",
    "\n",
    "print(\"Reranker ready âœ…\", type(reranker).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1755446997616,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "Z4xV2qRNyqsf",
    "outputId": "f2ec0f2f-41b9-44c2-d7ca-3a3391407032"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "def make_query_engine(use_llm_rerank=False):\n",
    "    retriever = hybrid_retriever if (globals().get(\"hybrid_retriever\") is not None) else vector_retriever\n",
    "    # choose exactly one reranker (weâ€™ll keep the cross-encoder/custom one)\n",
    "    post = [reranker]\n",
    "    return RetrieverQueryEngine(retriever=retriever, node_postprocessors=post)\n",
    "\n",
    "qe = make_query_engine()\n",
    "print(\"Query engine ready âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1755447978925,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "m3rDP1jp2lxJ",
    "outputId": "75368c68-18ea-49f3-84e7-32fc32e91dae"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "qe = RetrieverQueryEngine(retriever=hybrid_retriever, node_postprocessors=[reranker])\n",
    "print(\"Query engine now using ManualHybridRetriever âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 1331,
     "status": "ok",
     "timestamp": 1755447044295,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "7oP0GNgBy4f1",
    "outputId": "dc9ac131-d4a5-4fac-81cf-94bd650b37af"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from llama_index.llms.gemini import Gemini  # youâ€™re using this wrapper already\n",
    "from llama_index.core import Settings\n",
    "\n",
    "def rewrite_query(user_query: str) -> str:\n",
    "    \"\"\"Use a short completion prompt (avoids role import differences).\"\"\"\n",
    "    prompt = (\n",
    "        \"Rewrite the user's query to maximize retrieval recall and precision for legal/financial PDFs. \"\n",
    "        \"Use concise synonyms/terminology and keep it brief.\\n\\n\"\n",
    "        f\"User query: {user_query}\\n\\nRewritten query:\"\n",
    "    )\n",
    "    return Settings.llm.complete(prompt).text.strip()\n",
    "\n",
    "def show_sources(resp):\n",
    "    for i, sn in enumerate(resp.source_nodes, 1):\n",
    "        name = sn.node.metadata.get(\"file_name\")\n",
    "        score = round((sn.score or 0), 3)\n",
    "        snippet = textwrap.shorten(sn.node.get_text().strip().replace(\"\\n\",\" \"), width=220)\n",
    "        print(f\"[{i}] score={score} file={name} â†’ {snippet}\")\n",
    "\n",
    "def ask(q: str, expand: bool = True):\n",
    "    q2 = rewrite_query(q) if expand else q\n",
    "    resp = qe.query(q2)\n",
    "    print(\"\\nQ:\", q, \"\\nQ_expanded:\", q2, \"\\n\")\n",
    "    print(\"Answer:\\n\", resp, \"\\n\")\n",
    "    show_sources(resp)\n",
    "    return resp\n",
    "\n",
    "# quick smoke test\n",
    "_ = ask(\"What are the penalties for late payments?\", expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 2420,
     "status": "error",
     "timestamp": 1755448285506,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "-NeC4Dga2-zT",
    "outputId": "b9bdf7b7-aa43-465e-90ac-d6d7fe362a8d"
   },
   "outputs": [],
   "source": [
    "# Switch to a lighter model + gentle backoff to avoid 429s\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import Settings\n",
    "Settings.llm = Gemini(model=\"gemini-1.5-flash\", temperature=0.2, max_tokens=512)\n",
    "\n",
    "import time\n",
    "def ask_slow(q, expand=True):\n",
    "    time.sleep(1)   # tiny pause helps with rate limits\n",
    "    return ask(q, expand=expand)\n",
    "\n",
    "# Example:\n",
    "_ = ask_slow(\"What are the penalties for late payments?\", expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1755447509050,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "qv3LUfo-0rT0"
   },
   "outputs": [],
   "source": [
    "def set_retrieval(alpha: float = None, vector_top_k: int = None, bm25_top_k: int = None, use_llm_rerank: bool = False):\n",
    "    \"\"\"Adjust hybrid weight and top_k, then rebuild qe.\"\"\"\n",
    "    # Update top_k on existing retrievers if supported\n",
    "    if vector_top_k is not None and hasattr(vector_retriever, \"similarity_top_k\"):\n",
    "        vector_retriever.similarity_top_k = vector_top_k\n",
    "    if bm25_top_k is not None and hasattr(bm25_retriever, \"similarity_top_k\"):\n",
    "        bm25_retriever.similarity_top_k = bm25_top_k\n",
    "\n",
    "    # Rebuild HybridRetriever if available and alpha provided\n",
    "    if (alpha is not None) and (globals().get(\"hybrid_retriever\") is not None):\n",
    "        try:\n",
    "            from llama_index.core.retrievers import HybridRetriever\n",
    "            globals()[\"hybrid_retriever\"] = HybridRetriever(\n",
    "                vector_retriever=vector_retriever,\n",
    "                bm25_retriever=bm25_retriever,\n",
    "                alpha=float(alpha)\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Rebuild query engine\n",
    "    global qe\n",
    "    qe = make_query_engine(use_llm_rerank=use_llm_rerank)\n",
    "    print(\"Retrieval set â†’\",\n",
    "          f\"alpha={alpha if alpha is not None else 'unchanged'};\",\n",
    "          f\"vec@k={getattr(vector_retriever,'similarity_top_k',None)};\",\n",
    "          f\"bm25@k={getattr(bm25_retriever,'similarity_top_k',None)};\",\n",
    "          f\"rerank={'LLM' if use_llm_rerank else 'CE'}\")\n",
    "\n",
    "# Examples:\n",
    "# set_retrieval(alpha=0.3)              # lean more on keywords\n",
    "# set_retrieval(alpha=0.7)              # lean more on semantics\n",
    "# set_retrieval(vector_top_k=8)         # pull more chunks before rerank\n",
    "# set_retrieval(bm25_top_k=8)\n",
    "# set_retrieval(use_llm_rerank=True)    # try LLM reranking instead of cross-encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "error",
     "timestamp": 1755447674999,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "eI8I83G31bjA",
    "outputId": "d87b2811-5348-4ae5-e8ee-8aba6f6f72bf"
   },
   "outputs": [],
   "source": [
    "# --- Task 1 Validator ---\n",
    "import os, textwrap\n",
    "from llama_index.core import Settings\n",
    "\n",
    "def task1_validate(sample_q=\"What are the penalties for late payments?\"):\n",
    "    checks = {\n",
    "        \"GOOGLE_API_KEY_set\": bool(os.environ.get(\"GOOGLE_API_KEY\")),\n",
    "        \"LLM_ready\": hasattr(Settings, \"llm\") and Settings.llm is not None,\n",
    "        \"PDFs_loaded\": \"pdf_paths\" in globals() and len(pdf_paths) > 0,\n",
    "        \"rewrite_query_defined\": \"rewrite_query\" in globals(),\n",
    "        \"vector_retriever\": \"vector_retriever\" in globals(),\n",
    "        \"bm25_retriever\": \"bm25_retriever\" in globals(),\n",
    "        \"hybrid_retriever_present\": globals().get(\"hybrid_retriever\") is not None,\n",
    "        \"reranker_present\": \"reranker\" in globals(),\n",
    "        \"query_engine_ready\": \"qe\" in globals(),\n",
    "    }\n",
    "\n",
    "    # Run one query end-to-end and show top sources\n",
    "    print(\"â€” Task 1 Checklist â€”\")\n",
    "    for k,v in checks.items():\n",
    "        print(f\"{k:26} -> {'PASS' if v else 'MISSING'}\")\n",
    "\n",
    "    if checks[\"query_engine_ready\"] and checks[\"rewrite_query_defined\"]:\n",
    "        q_expanded = rewrite_query(sample_q)\n",
    "        print(\"\\nExpanded:\", q_expanded)\n",
    "        resp = qe.query(q_expanded)\n",
    "        print(\"\\nAnswer:\\n\", resp, \"\\n\")\n",
    "        print(\"Top sources:\")\n",
    "        for i, sn in enumerate(resp.source_nodes, 1):\n",
    "            name = sn.node.metadata.get(\"file_name\")\n",
    "            score = round((sn.score or 0), 3)\n",
    "            snippet = textwrap.shorten(sn.node.get_text().strip().replace(\"\\n\",\" \"), width=220)\n",
    "            print(f\"[{i}] score={score} file={name} â†’ {snippet}\")\n",
    "\n",
    "task1_validate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1755448369156,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "z-AqSGw54EM2",
    "outputId": "f62aa85e-d048-4552-83cf-528d413a4910"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# switch to lighter model\n",
    "Settings.llm = Gemini(model=\"gemini-1.5-flash\", temperature=0.15, max_tokens=256)\n",
    "\n",
    "# use your existing retrievers/reranker\n",
    "retriever = hybrid_retriever if ('hybrid_retriever' in globals() and hybrid_retriever is not None) else vector_retriever\n",
    "post = [reranker] if 'reranker' in globals() else []\n",
    "\n",
    "qe = RetrieverQueryEngine(retriever=retriever, node_postprocessors=post)\n",
    "print(\"Query engine rebuilt on gemini-1.5-flash âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 1413,
     "status": "ok",
     "timestamp": 1755448384703,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "8kgo675U4IlB",
    "outputId": "1faff6ba-0a60-4693-9b30-8832dcd2c278"
   },
   "outputs": [],
   "source": [
    "import time, textwrap\n",
    "\n",
    "def ask_slow(q: str, expand: bool = True, retries: int = 3, base_sleep: float = 1.2):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            q2 = rewrite_query(q) if expand else q\n",
    "            resp = qe.query(q2)\n",
    "            print(\"\\nQ:\", q, \"\\nQ_expanded:\", q2, \"\\n\")\n",
    "            print(\"Answer:\\n\", resp, \"\\n\")\n",
    "            for j, sn in enumerate(resp.source_nodes, 1):\n",
    "                name = sn.node.metadata.get(\"file_name\")\n",
    "                score = round((sn.score or 0), 3)\n",
    "                snippet = textwrap.shorten(sn.node.get_text().strip().replace(\"\\n\",\" \"), width=220)\n",
    "                print(f\"[{j}] score={score} file={name} â†’ {snippet}\")\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            if \"TooManyRequests\" in str(e) or \"429\" in str(e):\n",
    "                sleep_s = base_sleep * (2**i)\n",
    "                print(f\"Rate limit hit (429). Retrying in {sleep_s:.1f}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# try a query\n",
    "_ = ask_slow(\"What are the penalties for late payments?\", expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1755448426059,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "nkrcompg4TDo",
    "outputId": "97675281-b2d9-4b3c-f994-bb4a1468c11e"
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "vector_index.storage_context.persist(persist_dir=\"mortgage_index\")\n",
    "\n",
    "# Later / new runtime â€” reload\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage = StorageContext.from_defaults(persist_dir=\"mortgage_index\")\n",
    "vector_index = load_index_from_storage(storage)\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1755451234871,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "5QRk7woVC-xk",
    "outputId": "df658e51-acba-46c2-c9ac-bc8fccf1ecb6"
   },
   "outputs": [],
   "source": [
    "# One-shot: align settings + write final observations markdown\n",
    "from pathlib import Path\n",
    "\n",
    "# --- (Optional) align runtime settings to match notes ---\n",
    "try:\n",
    "    bm25_retriever.similarity_top_k = 6\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # If you used a manual hybrid retriever, set alpha here\n",
    "    hybrid_retriever.alpha = 0.5\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Rebuild QE so changes take effect (safe no-op if pieces aren't present)\n",
    "try:\n",
    "    from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "    retriever = hybrid_retriever if ('hybrid_retriever' in globals() and hybrid_retriever is not None) else vector_retriever\n",
    "    post = [reranker] if 'reranker' in globals() else []\n",
    "    qe = RetrieverQueryEngine(retriever=retriever, node_postprocessors=post)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Final observations content ---\n",
    "content = \"\"\"# Task 1 - Observations & Notes (Updated)\n",
    "\n",
    "## 1) Setup & Data\n",
    "- PDFs used: LenderFeesWorksheetNew.pdf\n",
    "- Embedding model: BAAI/bge-small-en-v1.5\n",
    "- Chunking: `chunk_size=1024`, `chunk_overlap=200`\n",
    "- LLM: `gemini-1.5-flash` (used for query expansion + answers)\n",
    "- Note: This PDF is a lender fee worksheet (costs/line items), not the promissory note or security instrument.\n",
    "\n",
    "## 2) Query Expansion\n",
    "Used Gemini to rewrite questions into retrieval-friendly phrasings.\n",
    "\n",
    "**Examples**\n",
    "- Original: *What are the penalties for late payments?*\n",
    "  Expanded: *Locate any late charge, delinquency fee, grace period, or default penalty terms.*\n",
    "- Original: *List borrower obligations and related deadlines.*\n",
    "  Expanded: *Summarize borrower covenants and due dates: payment due date, grace period, escrow/insurance requirements, disclosures, prepayment notice.*\n",
    "\n",
    "**Observation**\n",
    "Expansion added contract terminology (late charge, grace period, covenants), which improved recall vs plain wording.\n",
    "\n",
    "## 3) Hybrid Retrieval\n",
    "Manual hybrid fused BM25 (keywords) + vector (embeddings).\n",
    "- Vector @k: **6**\n",
    "- BM25 @k: **6** (increased so keyword matches participate fairly)\n",
    "- Fusion alpha (0=keywords, 1=semantic): **0.5**\n",
    "\n",
    "**Observation**\n",
    "Lower alpha (0.3) favored exact phrase sections; higher alpha (0.7) broadened context. 0.5 was a good balance for this doc.\n",
    "\n",
    "## 4) Reranking\n",
    "- Reranker: `CEPostprocessor` (cross-encoder: `ms-marco-MiniLM-L-6-v2`)\n",
    "- top_n: **3**\n",
    "\n",
    "**Observation**\n",
    "Reranking pushed the most on-topic snippet to the top and reduced duplicates.\n",
    "\n",
    "## 5) Results (Grounded)\n",
    "\n",
    "**Q1: What are the penalties for late payments?**\n",
    "Expanded: *Locate any late charge, delinquency fee, grace period, or default penalty terms.*\n",
    "**Finding:** The fee worksheet lists upfront/closing costs only. A search for \"late charge\", \"late fee\", \"grace period\", and \"delinquen*\" returned no hits in this PDF. Therefore, late-payment penalties are **not specified here** (they are typically in the **Note** or **Security Instrument**, not the fees worksheet).\n",
    "**Top sources (snippets):**\n",
    "- LenderFeesWorksheetNew.pdf â€” \"*...Fee Details and Summary...*\" (fee table context; no penalty terms)\n",
    "\n",
    "**Q2: List borrower obligations and related deadlines.**\n",
    "Expanded: *Summarize borrower covenants and due dates: payment due date, grace period, escrow/insurance requirements, disclosures, prepayment notice.*\n",
    "**Finding:** The worksheet enumerates **fees** owed (e.g., underwriting, appraisal, title, recording, insurance premium, per-diem interest). It does **not** specify deadlines or borrower covenants (e.g., escrow maintenance, insurance proof dates).\n",
    "**Top sources (snippets):**\n",
    "- LenderFeesWorksheetNew.pdf â€” \"*...underwriting fee, appraisal fee, lender's title insurance...*\"\n",
    "\n",
    "## 6) Errors, Limits, and Fixes\n",
    "- 429 TooManyRequests: mitigated with `gemini-1.5-flash`, lower `max_tokens`, and exponential backoff (`ask_slow`).\n",
    "- Built-in `HybridRetriever` missing: used a manual hybrid fusion of BM25+Vector.\n",
    "- BM25 signature differences across versions: tried `documents` -> `nodes` -> docstore fallback.\n",
    "- LlamaIndex `postprocessor` missing: used a cross-encoder fallback reranker.\n",
    "- Pip \"jedi\" warning: resolved by installing `jedi`; cosmetic otherwise.\n",
    "\n",
    "## 7) Next Steps\n",
    "- Tune alpha per query type (0.3 for exact phrases, 0.7 for semantic recall).\n",
    "- Increase `vector_top_k` to 8 on larger docs, then let the reranker filter.\n",
    "- Consider OCR/table extraction if future PDFs are scanned or tabular.\n",
    "- Optional: migrate to the newer Google GenAI integration when convenient.\n",
    "\"\"\"\n",
    "\n",
    "Path(\"Task1_Observations.md\").write_text(content, encoding=\"utf-8\")\n",
    "print(\"Wrote Task1_Observations.md â€” check the Files pane to open/download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6061,
     "status": "ok",
     "timestamp": 1755451782379,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "aO8V8ktnEHLW",
    "outputId": "ede8615d-cb94-486f-e3d0-031f9d93eb22"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # approve once\n",
    "\n",
    "# Choose a folder name in your Drive\n",
    "SAVE_DIR = \"/content/drive/MyDrive/llamaindex-task1\"\n",
    "\n",
    "import os, shutil\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "to_copy = [\n",
    "    \"Task1_Completed.ipynb\",\n",
    "    \"Task1_Observations.md\",\n",
    "    # include your saved index if you want the exact state reproducible:\n",
    "    # the folder was 'mortgage_index' if you created it earlier\n",
    "]\n",
    "for p in to_copy:\n",
    "    if os.path.exists(p):\n",
    "        shutil.copytree(p, f\"{SAVE_DIR}/{p}\", dirs_exist_ok=True) if os.path.isdir(p) \\\n",
    "        else shutil.copy2(p, SAVE_DIR)\n",
    "\n",
    "print(\"Copied to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1755451819341,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "xK29uqQvFPer",
    "outputId": "76a68439-4c75-4ac0-eec5-ff30a0538fa6"
   },
   "outputs": [],
   "source": [
    "# create a folder in your Drive and copy files into it\n",
    "!mkdir -p \"/content/drive/MyDrive/llamaindex-task1\"\n",
    "!cp Task1_Completed.ipynb Task1_Observations.md \"/content/drive/MyDrive/llamaindex-task1\"/\n",
    "# (optional) include your saved index\n",
    "# !cp -r mortgage_index \"/content/drive/MyDrive/llamaindex-task1/\"\n",
    "\n",
    "# verify\n",
    "!ls -al \"/content/drive/MyDrive/llamaindex-task1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1755452071036,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "JWv6px7EF6BB",
    "outputId": "77be43f9-409d-4b9b-a869-ead668814b34"
   },
   "outputs": [],
   "source": [
    "# list any .ipynb files that were just saved to your Drive root\n",
    "!ls -1 \"/content/drive/MyDrive\" | sed -n 's/.*\\.ipynb$/\\/content\\/drive\\/MyDrive\\/&/p'\n",
    "\n",
    "# replace <FILENAME.ipynb> with the exact name printed above\n",
    "SRC = \"/content/drive/MyDrive/<FILENAME.ipynb>\"\n",
    "DST_DIR = \"/content/drive/MyDrive/llamaindex-task1\"\n",
    "DST = f\"{DST_DIR}/Task1_Completed.ipynb\"\n",
    "\n",
    "!mkdir -p \"$DST_DIR\"\n",
    "!mv \"$SRC\" \"$DST\"\n",
    "!ls -al \"$DST_DIR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1593,
     "status": "ok",
     "timestamp": 1755453683326,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "o1Ky0YSaMT8v",
    "outputId": "2dbbb396-edea-4e78-dfb7-775153f88f01"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1755453698547,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "p9E754VSMZDi",
    "outputId": "b5a3daeb-c0c8-430c-ccb2-a04b23f5271a"
   },
   "outputs": [],
   "source": [
    "!find \"/content/drive/MyDrive\" -maxdepth 3 -name \"*.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1755453713731,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "0DtSq8_4Mcgl",
    "outputId": "f292ac59-9814-4a40-e4be-cbc9fe6d5eaf"
   },
   "outputs": [],
   "source": [
    "SRC = \"/content/drive/MyDrive/Colab Notebooks/Copy of YourNotebook.ipynb\"  # paste your path here\n",
    "DST_DIR = \"/content/drive/MyDrive/llamaindex-task1\"\n",
    "\n",
    "!mkdir -p \"$DST_DIR\"\n",
    "!cp \"$SRC\" \"$DST_DIR/Task1_Completed.ipynb\"\n",
    "!ls -al \"$DST_DIR\"\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNEutIlEoRf9Zhq+c13Lw/g",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
