{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129355,
     "status": "ok",
     "timestamp": 1755460457822,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "VV_K1o_1lHQ-",
    "outputId": "a9ee081c-f3fb-4721-f116-b1104579eadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m669.3/669.3 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \\\n",
    "  llama-index llama-index-llms-gemini \\\n",
    "  llama-index-retrievers-bm25 llama-index-embeddings-huggingface \\\n",
    "  sentence-transformers pymupdf jedi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "executionInfo": {
     "elapsed": 72196,
     "status": "ok",
     "timestamp": 1755460637715,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "y-CBOa8Ml3bY",
    "outputId": "db371a5a-e41a-4041-cf25-88db13b92ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Paste your Gemini API key (hidden): ··········\n",
      "Key loaded ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-134182287.py:14: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  Settings.llm = Gemini(model=\"gemini-1.5-flash\", temperature=0.15, max_tokens=256)\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings ready ✅\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# set key once per runtime (hidden prompt)\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"🔑 Paste your Gemini API key (hidden): \").strip()\n",
    "print(\"Key loaded ✅\")\n",
    "\n",
    "# light, quota-friendly defaults\n",
    "Settings.llm = Gemini(model=\"gemini-1.5-flash\", temperature=0.15, max_tokens=256)\n",
    "Settings.embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "print(\"Settings ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 21596,
     "status": "ok",
     "timestamp": 1755460670674,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "Cx5Ihf6bl6ZW",
    "outputId": "f1af69a9-d9c9-49d3-b164-541a80523496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the Lender Fee Worksheet PDF…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LenderFeesWorksheetNew (3).pdf to LenderFeesWorksheetNew (3).pdf\n",
      "Loaded 1 document(s)\n"
     ]
    }
   ],
   "source": [
    "PDF = \"LenderFeesWorksheetNew.pdf\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(PDF):\n",
    "    from google.colab import files\n",
    "    print(\"Select the Lender Fee Worksheet PDF…\")\n",
    "    uploaded = files.upload()\n",
    "    PDF = list(uploaded.keys())[0]\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(input_files=[PDF]).load_data()\n",
    "print(f\"Loaded {len(docs)} document(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "executionInfo": {
     "elapsed": 4552,
     "status": "ok",
     "timestamp": 1755460684442,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "uUnJD5S5l65z",
    "outputId": "08bc63c6-7100-4d23-bab0-2b1d0deea00b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bm25s:Building index from IDs objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector@k=6 | BM25@k=1 ✅\n",
      "Hybrid retriever ready ✅\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Vector\n",
    "vector_index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=6)\n",
    "\n",
    "# Prepare nodes for BM25 variants\n",
    "try:\n",
    "    nodes = Settings.node_parser.get_nodes_from_documents(docs)\n",
    "except Exception:\n",
    "    nodes = None\n",
    "\n",
    "# BM25 import (old/new paths)\n",
    "try:\n",
    "    from llama_index.retrievers.bm25 import BM25Retriever\n",
    "except Exception:\n",
    "    from llama_index.core.retrievers import BM25Retriever\n",
    "\n",
    "bm25 = None; last_err = None\n",
    "# A) modern signature\n",
    "try:\n",
    "    bm25 = BM25Retriever.from_defaults(documents=docs, similarity_top_k=6)\n",
    "except Exception as e:\n",
    "    last_err = e\n",
    "# B) nodes signature\n",
    "if bm25 is None and nodes:\n",
    "    try:\n",
    "        bm25 = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=min(6, len(nodes)))\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "# C) docstore fallback\n",
    "if bm25 is None:\n",
    "    from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "    ds = SimpleDocumentStore()\n",
    "    if hasattr(ds, \"add_documents\"): ds.add_documents(docs)\n",
    "    elif hasattr(ds, \"add_nodes\") and nodes: ds.add_nodes(nodes)\n",
    "    bm25 = BM25Retriever.from_defaults(docstore=ds, similarity_top_k=6)\n",
    "\n",
    "# Clamp k so tiny PDFs don’t crash BM25\n",
    "try:\n",
    "    corp_size = len(nodes) if nodes else 1\n",
    "    bm25.similarity_top_k = max(1, min(getattr(bm25, \"similarity_top_k\", 6), corp_size))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"Vector@k={getattr(vector_retriever,'similarity_top_k',None)} | BM25@k={getattr(bm25,'similarity_top_k',None)} ✅\")\n",
    "\n",
    "# Manual hybrid (works across all versions)\n",
    "try:\n",
    "    from llama_index.core.retrievers import BaseRetriever\n",
    "except Exception:\n",
    "    BaseRetriever = object\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class ManualHybridRetriever(BaseRetriever):\n",
    "    def __init__(self, v, b, alpha=0.5, k=6):\n",
    "        self.v=v; self.b=b; self.alpha=float(alpha); self.k=int(k)\n",
    "    def _norm(self, items):\n",
    "        sc=[(i.score or 0.0) for i in items] or [0.0]\n",
    "        mn, mx = min(sc), max(sc); rng=(mx-mn) or 1.0\n",
    "        return {id(i.node):((i.score or 0.0)-mn)/rng for i in items}\n",
    "    async def _aretrieve(self, q): return self._retrieve(q)\n",
    "    def _retrieve(self, q):\n",
    "        vec=self.v.retrieve(q); kw=self.b.retrieve(q)\n",
    "        nv=self._norm(vec); nk=self._norm(kw)\n",
    "        merged={}\n",
    "        for lst in (vec,kw):\n",
    "            for nws in lst: merged.setdefault(id(nws.node), nws)\n",
    "        out=[]\n",
    "        for nid,nws in merged.items():\n",
    "            score=self.alpha*nv.get(nid,0.0)+(1-self.alpha)*nk.get(nid,0.0)\n",
    "            out.append(NodeWithScore(node=nws.node, score=score))\n",
    "        out.sort(key=lambda x: x.score or 0.0, reverse=True)\n",
    "        return out[:self.k]\n",
    "\n",
    "hybrid_retriever = ManualHybridRetriever(vector_retriever, bm25, alpha=0.5, k=6)\n",
    "print(\"Hybrid retriever ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "executionInfo": {
     "elapsed": 3534,
     "status": "ok",
     "timestamp": 1755460694144,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "r3mLf8ixl7dD",
    "outputId": "860b3eb8-8981-483a-a1d3-e67a18b1c6b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker ready ✅ CEPostprocessor\n"
     ]
    }
   ],
   "source": [
    "reranker = None\n",
    "try:\n",
    "    from llama_index.postprocessor import SentenceTransformerRerank\n",
    "    reranker = SentenceTransformerRerank(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3)\n",
    "except Exception:\n",
    "    try:\n",
    "        from llama_index.postprocessor.sentence_transformer_rerank import SentenceTransformerRerank\n",
    "        reranker = SentenceTransformerRerank(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3)\n",
    "    except Exception:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        class CEPostprocessor:\n",
    "            def __init__(self, model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3):\n",
    "                self.ce = CrossEncoder(model); self.top_n = top_n\n",
    "            def postprocess_nodes(self, nodes, query_bundle):\n",
    "                q = getattr(query_bundle,\"query_str\",str(query_bundle))\n",
    "                txts=[]\n",
    "                for n in nodes:\n",
    "                    node = getattr(n,\"node\",n)\n",
    "                    txts.append(node.get_text() if hasattr(node,\"get_text\") else str(getattr(n,\"text\",\"\")))\n",
    "                scores = self.ce.predict([[q,t] for t in txts])\n",
    "                for sc,n in zip(scores,nodes):\n",
    "                    try: n.score=float(sc)\n",
    "                    except: pass\n",
    "                ranked=[n for _,n in sorted(zip(scores,nodes), key=lambda t:t[0], reverse=True)]\n",
    "                return ranked[:self.top_n]\n",
    "        reranker = CEPostprocessor(top_n=3)\n",
    "print(\"Reranker ready ✅\", type(reranker).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "id": "gI2PpmhLmJxG"
   },
   "outputs": [],
   "source": [
    "import re, time, textwrap\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import Settings\n",
    "\n",
    "qe = RetrieverQueryEngine(retriever=hybrid_retriever, node_postprocessors=[reranker])\n",
    "\n",
    "def rewrite_query(q: str) -> str:\n",
    "    prompt = (\"Rewrite the user's question for precise retrieval in a lender fee worksheet. \"\n",
    "              \"Add key synonyms; keep it short.\\n\\n\"\n",
    "              f\"User: {q}\\nRewritten:\")\n",
    "    return Settings.llm.complete(prompt).text.strip()\n",
    "\n",
    "def ask_rag(q: str, expand=True, retries=3, base_sleep=1.2):\n",
    "    q2 = rewrite_query(q) if expand else q\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            resp = qe.query(q2)\n",
    "            print(f\"\\nQ: {q}\\nQ_expanded: {q2}\\n\")\n",
    "            print(\"Answer:\\n\", resp, \"\\n\")\n",
    "            print(\"Top sources:\")\n",
    "            for j, sn in enumerate(resp.source_nodes, 1):\n",
    "                node = getattr(sn,\"node\",sn)\n",
    "                name = getattr(node,\"metadata\",{}).get(\"file_name\",\"(unknown)\")\n",
    "                score = round((getattr(sn,\"score\",0) or 0), 3)\n",
    "                text = node.get_text() if hasattr(node,\"get_text\") else str(getattr(sn,\"text\",\"\"))\n",
    "                snippet = textwrap.shorten(text.replace(\"\\n\",\" \"), width=220)\n",
    "                print(f\"[{j}] score={score} file={name}\\n    {snippet}\")\n",
    "            return str(resp), resp.source_nodes\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) or \"TooManyRequests\" in str(e):\n",
    "                sleep = base_sleep*(2**i)\n",
    "                print(f\"Rate limit: retrying in {sleep:.1f}s…\")\n",
    "                time.sleep(sleep)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def extract_amount(sources, keywords):\n",
    "    \"\"\"Pick a $ amount near given keywords from top sources.\"\"\"\n",
    "    joined=\"\"\n",
    "    for sn in sources[:3]:\n",
    "        node = getattr(sn,\"node\",sn)\n",
    "        joined += \"\\n\" + (node.get_text() if hasattr(node,\"get_text\") else str(getattr(sn,\"text\",\"\")))\n",
    "    best=None\n",
    "    for line in joined.splitlines():\n",
    "        lo=line.lower()\n",
    "        if any(k in lo for k in keywords):\n",
    "            for m in re.findall(r\"\\$?\\s?\\d[\\d,]*(?:\\.\\d{2})?\", line):\n",
    "                best=m\n",
    "    if not best:\n",
    "        m=re.search(r\"\\$?\\s?\\d[\\d,]*(?:\\.\\d{2})?\", joined)\n",
    "        best=m.group(0) if m else None\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818
    },
    "executionInfo": {
     "elapsed": 3531,
     "status": "ok",
     "timestamp": 1755460710518,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "P3AI03WimKLf",
    "outputId": "288d0e1e-9e6f-43bb-a047-8c0b3eecb014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is the total estimated monthly payment?\n",
      "Q_expanded: Total monthly payment; Estimated payment; Monthly payment amount; Total payment due monthly\n",
      "\n",
      "Answer:\n",
      " The estimated monthly payment is $1,869.37.\n",
      " \n",
      "\n",
      "Top sources:\n",
      "[1] score=0.254 file=LenderFeesWorksheetNew (3).pdf\n",
      "    Your actual rate, payment, and cost could be higher. Get an official Loan Estimate before choosing a loan. Fee Details and Summary Applicants: Application No: Date Prepared: Loan Program: Prepared By: THIS IS NOT A [...]\n",
      "\n",
      "Estimated monthly payment (best evidence): 09\n",
      "\n",
      "Q: How much does the borrower pay for lender's title insurance?\n",
      "Q_expanded: Lender's title insurance cost; borrower's title insurance fee\n",
      "\n",
      "Answer:\n",
      " The lender's title insurance cost is not specified.  The borrower's title insurance fee is $650.00.\n",
      " \n",
      "\n",
      "Top sources:\n",
      "[1] score=-3.142 file=LenderFeesWorksheetNew (3).pdf\n",
      "    Your actual rate, payment, and cost could be higher. Get an official Loan Estimate before choosing a loan. Fee Details and Summary Applicants: Application No: Date Prepared: Loan Program: Prepared By: THIS IS NOT A [...]\n",
      "\n",
      "Lender's title insurance (best evidence): $ 475.00\n",
      "\n",
      "============================================================\n",
      "Short Explanation of Design Choices:\n",
      "- Embeddings: BAAI/bge-small-en-v1.5 — fast & strong for English retrieval.\n",
      "- Chunking: 1024 tokens, 200 overlap — balances recall with context.\n",
      "- Retrieval: Hybrid (BM25 + Vector, α=0.5) — exact fee names + semantic recall; cross-encoder reranker for precision.\n",
      "============================================================\n",
      "\n",
      "Response to Prompt 1:\n",
      " The estimated monthly payment is $1,869.37.\n",
      " \n",
      "Evidence amount: 09\n",
      "\n",
      "Response to Prompt 2:\n",
      " The lender's title insurance cost is not specified.  The borrower's title insurance fee is $650.00.\n",
      " \n",
      "Evidence amount: $ 475.00\n",
      "\n",
      "(If an amount is None, the PDF likely doesn’t list it explicitly—your answer text + sources still count.)\n"
     ]
    }
   ],
   "source": [
    "# Prompt 1\n",
    "ans1, src1 = ask_rag(\"What is the total estimated monthly payment?\")\n",
    "amt1 = extract_amount(src1, keywords=(\"monthly\",\"payment\",\"estimated\",\"total\"))\n",
    "print(\"\\nEstimated monthly payment (best evidence):\", amt1)\n",
    "\n",
    "# Prompt 2\n",
    "ans2, src2 = ask_rag(\"How much does the borrower pay for lender's title insurance?\")\n",
    "amt2 = extract_amount(src2, keywords=(\"lender\",\"title\",\"insurance\",\"premium\"))\n",
    "print(\"\\nLender's title insurance (best evidence):\", amt2)\n",
    "\n",
    "# Final deliverables text (copy-paste into your submission form)\n",
    "choices = (\n",
    "  \"- Embeddings: BAAI/bge-small-en-v1.5 — fast & strong for English retrieval.\\n\"\n",
    "  \"- Chunking: 1024 tokens, 200 overlap — balances recall with context.\\n\"\n",
    "  \"- Retrieval: Hybrid (BM25 + Vector, α=0.5) — exact fee names + semantic recall; cross-encoder reranker for precision.\"\n",
    ")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Short Explanation of Design Choices:\\n\" + choices)\n",
    "print(\"=\"*60)\n",
    "print(\"\\nResponse to Prompt 1:\\n\", ans1, f\"\\nEvidence amount: {amt1}\")\n",
    "print(\"\\nResponse to Prompt 2:\\n\", ans2, f\"\\nEvidence amount: {amt2}\")\n",
    "print(\"\\n(If an amount is None, the PDF likely doesn’t list it explicitly—your answer text + sources still count.)\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO5nehKEHYTIG6WANnkPdcI",
   "provenance": [
    {
     "file_id": "1dakixxCj54n_BsOH5DsBQuGC3UnpzhJj",
     "timestamp": 1755460933009
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
