{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129355,
     "status": "ok",
     "timestamp": 1755460457822,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "VV_K1o_1lHQ-",
    "outputId": "a9ee081c-f3fb-4721-f116-b1104579eadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m669.3/669.3 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \\\n",
    "  llama-index llama-index-llms-gemini \\\n",
    "  llama-index-retrievers-bm25 llama-index-embeddings-huggingface \\\n",
    "  sentence-transformers pymupdf jedi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580,
     "referenced_widgets": [
      "669ada5c11224bd1852a5e5703716fc6",
      "edbdc74fb57441ca87c8f3aea7fd59ee",
      "0070bc489e7142639b54d257e8bcafbb",
      "e0189a1f16b74a17bb91e48e27927296",
      "b088b18143ce40f5afb88aceae9c7b36",
      "2e2a298ef0f44dd7af1218ae62b63b21",
      "9d0b042fc94a459382d55130019c6b32",
      "9c251665e7fc4264b5686d18b847875a",
      "99cb85257f1b4426b4e713beeb550530",
      "951a3ad6e8f3432fb38aea43f9a9b447",
      "1433b62d645e40759d4a20f28a396298",
      "a418880be7024ad18bf11a25ee4ef7aa",
      "26af2a18066d4fa1896ccb1848573770",
      "51c4d2873ca54b0bafbd468c4209a39c",
      "d582ba9d038b49688df02f2f20c6efe8",
      "4e214942e53446309d3af9e544b2e14a",
      "2627f7b318304791b10cbc8cf163ecc2",
      "48320d211e4940a9a4a2e7292815f3ff",
      "eef9c6e4aed141fca7b3c6c2555706aa",
      "8130845f25cd4be1bd2af4884c987a6e",
      "ae352905e9e9499ea0382acf14a97c03",
      "8ebc0b5334784c0ca9507c65c40874dc",
      "0c151fd192644eb1accb87e70daf918b",
      "a96b9db0429849ea868f0ea9082a2762",
      "d54f1fb3dd9641eeb803346fdf58a549",
      "b659778e331146d4805a5d7d2e90c5d6",
      "f3f1ec09e69f41afac6edff0492b70fb",
      "eb010a4335774d73b77d99fb8ec55a87",
      "07124a13618749c292b77389b39328f7",
      "7c5e47861e684e7db95461da6ba020f7",
      "960ac7b6d3f64aa8a0e346eb5199756d",
      "16d33cd7cbc945a8bfd59a4ba51595eb",
      "9ac8dfa427ac4ca3becaae4234ae2e28",
      "83ec401fba55406bbbaa7686514b7209",
      "6657706321734bddb4f9ad3ef11cf704",
      "8a685b959eea499c98f65d70d6396a9f",
      "9d274f7ed0774a3b8dad01ab7133d397",
      "87ea1ae4b98e40c3bdb6e53b7a77a940",
      "2520da6b24ea4b37b1729f4d49a5c69f",
      "a036a514e4bc4a518fd4217d9852b7aa",
      "682cc05b8d6e46099db5ea68e083548d",
      "5c86e6023dcb401fad595e8377f623f5",
      "92002d57cbd545049ec3c36962ef0325",
      "051cb83eaad646bab9caa587d85f0e10",
      "8e15829b2bc44ca1adeb3b259063287e",
      "d929262bd1e24b19a5b6ba0460fa033a",
      "149d9eae7d6349bf954da4138c8f0207",
      "29453035ab644ae895eaec87f50284f9",
      "b146de424f064590a8dfed5be1befe40",
      "7b5ddfa47575470d9b59985ec8365305",
      "8abb6bf426864ff084ba0b55a9b6feaa",
      "e802072bbd4f4a31930d029a4abee978",
      "ee1a1da3e183481d99ba61fa55a77a74",
      "c6e48338027e49e99f84bbf0f8b3066f",
      "48f18d4ddaf343dd85b40e24767d11cf",
      "5bec370c9dd8413d8ae00a75e5e5ec14",
      "e36d6f598bde48568340bb59d571c894",
      "d30b8a30e993471b85e96d783546a48c",
      "d6a1acdab96447f381f5b8b15b744b6d",
      "57362f6a9442499d8efdb2c11e55cda8",
      "1a9b1d9ece4f430fb9cadcfa49e2164d",
      "365e2cf22ac64f79ba29ccea603ce06c",
      "0d2eedabe2d5407bb1cf694fec02f149",
      "b1ec78095be64e6292e901cf8294c6ad",
      "b4c3e9c1922846769d5f888f5a222095",
      "f6da4bf1d094466b892f6e2679376724",
      "4b9877ec03c64e6483a3aab385e7e665",
      "bca37e2e95bf4c43a45f2e26a2812593",
      "9c16b6ce255e4f0a971c96db359b7cba",
      "202d87b9985f403a9b99d6a562a98a4b",
      "b45d81d303474313853ccd213b795759",
      "57bcea457e4b476ab17fb4f87eb1b05c",
      "50e964ec0a604cef90d292b0bbb6fb51",
      "b22545f8f7944806bfc8d1e52bdf372e",
      "072334508e694b809887d23d74fc4e29",
      "fc974255e86f4ef1b489319343072482",
      "d564f20acc6d49fd8de4f4e6a2844e5e",
      "86c092badf4c4c8f89eba1db12d4799e",
      "0f25c483cda74b9f883f59895cf26923",
      "388c7ed3afaf4577b6492cd8e2b005ef",
      "42ed2bf1b46849ab9d5e05def8ad579f",
      "0d88e066573c4334bdb1b24b3b2622a6",
      "f4edf217ba82498eac2ee0159a24f8ed",
      "67bf193b85af4f3dbe4f4ef054704369",
      "3e8bc91259744ac4a2855cfd9db1435a",
      "ba1ff6020a594b4c906d1c514d6ac709",
      "e37ecfe2acb341e2a109328bc98cd1f3",
      "7293ecf24ed542a8aa09ec840f9c8fdf",
      "9efa916e837a484983f634d02bcda4aa",
      "a976dee624604c9db6fc87184c150a3f",
      "e2759c6903bc4a6e8ffe3ebbf334a5d8",
      "b610a5f1a92f4d179c3cdf2dfe491b69",
      "a0e4700c03b9475d9a0a66304c989f89",
      "e5ee4e128ba14ce5b69b2ce547575e3b",
      "6d7cec49c4674209ad7e3e42c1a5ef56",
      "b5fb3fa365814671835ab6babf5dbaf9",
      "cd2a24d28cd3482db197df231a0f0b48",
      "504ebae863154542a2e17776e47a9790",
      "6f386a7b771a4a14b9fb55c4ddcad9fd",
      "69f9c0f34e9448f0a90c176906c83de6",
      "940e3cd5539942d7864500cb547a5681",
      "fce6e21c3acc4e6e893eff1d3ca58756",
      "8ed0937174d94868a34733689fab4520",
      "25e27fbe82d24075bbad901677ca3a8b",
      "6f227217b1864370aa02572807401bc6",
      "af7b5d9982d640159d1f21c517faee8d",
      "bda272aeeb654114a9d92cbc9c39b501",
      "d59b4cee7c0a436797945b925f3b62d1",
      "d17f85eaef4a490db157e38263c1fc6c",
      "7b6208f6b1e54d4087a4c297b474986d",
      "b8de9317f4a944a98a000b00e4efe013",
      "65779cc295b54392b3b7d8723b1bdee2",
      "4fb3219fa28a4d1f9b4a90da2a661cac",
      "32e3faec78d84955b728f1c7225a3fe8",
      "18c0c3645a9e496796aaa8a12a5a147e",
      "c9c4e02bb66d4cd797526763fd2c6ad4",
      "5283430e55834e598b3105f0a7dd183d",
      "6f70efe79a394ea28fbca284824a19fe",
      "fa3d7b2723ba4c349c2aa7d2b133e331",
      "2b5477eac05a4438af1b49dd2e6df8e0",
      "c7a9d99b595c466da53c0b7dbbb2d48c"
     ]
    },
    "executionInfo": {
     "elapsed": 72196,
     "status": "ok",
     "timestamp": 1755460637715,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "y-CBOa8Ml3bY",
    "outputId": "db371a5a-e41a-4041-cf25-88db13b92ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ Paste your Gemini API key (hidden): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "Key loaded âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-134182287.py:14: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  Settings.llm = Gemini(model=\"gemini-1.5-flash\", temperature=0.15, max_tokens=256)\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669ada5c11224bd1852a5e5703716fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a418880be7024ad18bf11a25ee4ef7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c151fd192644eb1accb87e70daf918b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ec401fba55406bbbaa7686514b7209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e15829b2bc44ca1adeb3b259063287e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bec370c9dd8413d8ae00a75e5e5ec14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9877ec03c64e6483a3aab385e7e665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c092badf4c4c8f89eba1db12d4799e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efa916e837a484983f634d02bcda4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f9c0f34e9448f0a90c176906c83de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8de9317f4a944a98a000b00e4efe013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings ready âœ…\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# set key once per runtime (hidden prompt)\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"ğŸ”‘ Paste your Gemini API key (hidden): \").strip()\n",
    "print(\"Key loaded âœ…\")\n",
    "\n",
    "# light, quota-friendly defaults\n",
    "Settings.llm = Gemini(model=\"gemini-1.5-flash\", temperature=0.15, max_tokens=256)\n",
    "Settings.embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "print(\"Settings ready âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 21596,
     "status": "ok",
     "timestamp": 1755460670674,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "Cx5Ihf6bl6ZW",
    "outputId": "f1af69a9-d9c9-49d3-b164-541a80523496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the Lender Fee Worksheet PDFâ€¦\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-da80ee97-cdd7-4ec0-b9d6-38f9657b7499\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-da80ee97-cdd7-4ec0-b9d6-38f9657b7499\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LenderFeesWorksheetNew (3).pdf to LenderFeesWorksheetNew (3).pdf\n",
      "Loaded 1 document(s)\n"
     ]
    }
   ],
   "source": [
    "PDF = \"LenderFeesWorksheetNew.pdf\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(PDF):\n",
    "    from google.colab import files\n",
    "    print(\"Select the Lender Fee Worksheet PDFâ€¦\")\n",
    "    uploaded = files.upload()\n",
    "    PDF = list(uploaded.keys())[0]\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(input_files=[PDF]).load_data()\n",
    "print(f\"Loaded {len(docs)} document(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133,
     "referenced_widgets": [
      "d4f9cfe3dc5944f299baa37098755907",
      "402aadb7ee894f9583d309c9dc178040",
      "ac89a7d6460240aa813c53c9b0a0c015",
      "6fe0924dc7cf4841b880264c12f52caa",
      "7ae4c22937a44ed690030afcf15afb12",
      "26218ffffe1b41768a8eaaa41007e151",
      "c6d920bf3bdc4b6a8d69f406419f312d",
      "c04f1704779a4e2585361f480104f389",
      "186981787eb44dc29ee05e0b12bfda33",
      "f8644da7bee94b6b8b56952030e50c52",
      "b852c197778448cba92c914a47e60180",
      "a12930d5ffca46b9b4bf1752332ad761",
      "283b0bca43874645941ad4843307a711",
      "e8c5b046fc4f40f48aee32498ebcebaf",
      "b78a69ac222e4262976b14f72dd88413",
      "969dd1a6b54a4b8f81f7952f0bc6d0f1",
      "a9ee23be55ca49f59e301d097854db8d",
      "65efa175ca9c42969dd7f91ccf8df702",
      "c7d48ea67b584e508bd86f65e62255e3",
      "8f91066c0596433db6983a17fdea14c0",
      "fa1d9288bb9042dcaa04ee0263c24dac",
      "8ab62d54700e42b89d997fc0812102a4"
     ]
    },
    "executionInfo": {
     "elapsed": 4552,
     "status": "ok",
     "timestamp": 1755460684442,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "uUnJD5S5l65z",
    "outputId": "08bc63c6-7100-4d23-bab0-2b1d0deea00b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f9cfe3dc5944f299baa37098755907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12930d5ffca46b9b4bf1752332ad761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bm25s:Building index from IDs objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector@k=6 | BM25@k=1 âœ…\n",
      "Hybrid retriever ready âœ…\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Vector\n",
    "vector_index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=6)\n",
    "\n",
    "# Prepare nodes for BM25 variants\n",
    "try:\n",
    "    nodes = Settings.node_parser.get_nodes_from_documents(docs)\n",
    "except Exception:\n",
    "    nodes = None\n",
    "\n",
    "# BM25 import (old/new paths)\n",
    "try:\n",
    "    from llama_index.retrievers.bm25 import BM25Retriever\n",
    "except Exception:\n",
    "    from llama_index.core.retrievers import BM25Retriever\n",
    "\n",
    "bm25 = None; last_err = None\n",
    "# A) modern signature\n",
    "try:\n",
    "    bm25 = BM25Retriever.from_defaults(documents=docs, similarity_top_k=6)\n",
    "except Exception as e:\n",
    "    last_err = e\n",
    "# B) nodes signature\n",
    "if bm25 is None and nodes:\n",
    "    try:\n",
    "        bm25 = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=min(6, len(nodes)))\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "# C) docstore fallback\n",
    "if bm25 is None:\n",
    "    from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "    ds = SimpleDocumentStore()\n",
    "    if hasattr(ds, \"add_documents\"): ds.add_documents(docs)\n",
    "    elif hasattr(ds, \"add_nodes\") and nodes: ds.add_nodes(nodes)\n",
    "    bm25 = BM25Retriever.from_defaults(docstore=ds, similarity_top_k=6)\n",
    "\n",
    "# Clamp k so tiny PDFs donâ€™t crash BM25\n",
    "try:\n",
    "    corp_size = len(nodes) if nodes else 1\n",
    "    bm25.similarity_top_k = max(1, min(getattr(bm25, \"similarity_top_k\", 6), corp_size))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"Vector@k={getattr(vector_retriever,'similarity_top_k',None)} | BM25@k={getattr(bm25,'similarity_top_k',None)} âœ…\")\n",
    "\n",
    "# Manual hybrid (works across all versions)\n",
    "try:\n",
    "    from llama_index.core.retrievers import BaseRetriever\n",
    "except Exception:\n",
    "    BaseRetriever = object\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class ManualHybridRetriever(BaseRetriever):\n",
    "    def __init__(self, v, b, alpha=0.5, k=6):\n",
    "        self.v=v; self.b=b; self.alpha=float(alpha); self.k=int(k)\n",
    "    def _norm(self, items):\n",
    "        sc=[(i.score or 0.0) for i in items] or [0.0]\n",
    "        mn, mx = min(sc), max(sc); rng=(mx-mn) or 1.0\n",
    "        return {id(i.node):((i.score or 0.0)-mn)/rng for i in items}\n",
    "    async def _aretrieve(self, q): return self._retrieve(q)\n",
    "    def _retrieve(self, q):\n",
    "        vec=self.v.retrieve(q); kw=self.b.retrieve(q)\n",
    "        nv=self._norm(vec); nk=self._norm(kw)\n",
    "        merged={}\n",
    "        for lst in (vec,kw):\n",
    "            for nws in lst: merged.setdefault(id(nws.node), nws)\n",
    "        out=[]\n",
    "        for nid,nws in merged.items():\n",
    "            score=self.alpha*nv.get(nid,0.0)+(1-self.alpha)*nk.get(nid,0.0)\n",
    "            out.append(NodeWithScore(node=nws.node, score=score))\n",
    "        out.sort(key=lambda x: x.score or 0.0, reverse=True)\n",
    "        return out[:self.k]\n",
    "\n",
    "hybrid_retriever = ManualHybridRetriever(vector_retriever, bm25, alpha=0.5, k=6)\n",
    "print(\"Hybrid retriever ready âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "referenced_widgets": [
      "532d0d53e2da44c9b85d0e01cfd8c331",
      "157c0656937b4929b52a6d6f3d34bc7d",
      "5f477bd332f74ecbb146709d0f2acbdc",
      "70f8bbcbd01448c285feba8de46548ef",
      "959b31f806f94ce0bc4a41854d0a4986",
      "68179f2bcb2a4b6d9b40fcea30bca3c9",
      "9a7cfe72b65e47528b14bcfbfec11f8f",
      "9abf082e0ea4415ca656dcb8e2e25980",
      "c14c351bcaa147b08f7a1bca736f56d0",
      "6cde2e6483e64f5db7f7085fe8a4f9c7",
      "35c822ef78394033afd6ca1dfc60ffab",
      "f22473934ea94c69b5c608c3b4e89abb",
      "302b9d97ee804b5289aa3acd9f4eb099",
      "256b1714b74046bfa08960cce47e7336",
      "c2a868417c8e499ca4124279fc8adbdf",
      "a7f5aa41e63145f4b2ac79b7e83b9492",
      "ee207789734f4d3eaea597f55492e774",
      "5097e56e382d4708902960a2182d1fa7",
      "ce25c4fb0ec042caa3830847ec45ea21",
      "3a7b67eb5e404bda801988de884fb3ad",
      "89032db22c5e453aa03513a34fdb9eee",
      "c1d1555fed5e4a0fba01a6902b1bbc75",
      "ef221d18b95d4a38ad1b543c1b09f605",
      "8cef8531daa741f8855e4d46fa14db24",
      "602a52c513b54c95bde5fbfc9edd60a8",
      "2009f8625f37453cb98991c1c61069ca",
      "9a7caa254c51437c8680a689625700e7",
      "4dab7ada18b2400bbe465d1417e45b89",
      "e91d8c82eb1047b1a3df4157ed75ecad",
      "a040fb870fd9478c889d503e072be484",
      "723bb080d9394d8babadb86bfaf7c94e",
      "bc455a994c3a4a19827808ffb06779d3",
      "6871a4c31b8a4d9f8604915fc378f069",
      "bfc55e92cae24888b56f627ef3023ea5",
      "ce47d0d2a37d45288e66782f52361544",
      "75277f4f69da4f018a6ecc553f0a0eca",
      "e3c745a6239e4196a3a3e25c4ec9f9f2",
      "91b9bd25dd404c3d90787fbead118798",
      "2b50562528264d1bbef6cbab1f49fd95",
      "6822a060289240d5b18b8cd19bd980bf",
      "09505b82ea6b40dca1ca7726a5b1f039",
      "f9f4992227f44c6a84f65b424474b90c",
      "6dea05ccafdb41eda308fe9a242318a0",
      "12353e830fd24fd998b06f02e082f7c7",
      "5995b41e9b1c4dd2bd68f02b996b726b",
      "9d664fb4008a4c8692f602ee8d53664e",
      "1caa1dfe0aa74705bd72635b4c826e43",
      "eb6b04c708bc4b259ef5f152a19c939d",
      "7235e77defbd4b12b9b60bde364b489e",
      "bef6f9bd90c944e3ab7eda2b5709bfeb",
      "7e66067e6ddb4983872ad403d7a467c4",
      "49e99e330691465aa14596ace0ed6557",
      "0060646804434f909022ee6c84ae57d7",
      "650c0197a197407ca86dfdec8649850a",
      "a060b7a9dac64d93837b1ccb30f68c63",
      "78e492bf41c646348b37e0d8a7c3ed16",
      "4ad66f599f2642d180322fc5bd68c1c7",
      "8680f617213e4ca68dcd35b2b689e726",
      "559b3a8eddc048a2a04aaa41f6621996",
      "b89319c6ca3b4f148de75da94dd91958",
      "84417ff7859945acb872b624e336c573",
      "e5be9f9242504ce3923f459771c747b3",
      "53b9e43f04a84e94a4e194069c511438",
      "b9fb3defd8ef413bb0d087225bdf9a5f",
      "c14ab3f3bbd14102be8a676cfbfaf326",
      "f2a24bfa4c6e4dcb9af975dca6cd06e8",
      "de609defe2ec43959a3bb3d82412cb0a",
      "b795ad7e672440938b3826c97f1d4a5e",
      "989860172aa64684a13a70c330c0cd10",
      "2b171ce38a5942b58c6faefc5004bf4a",
      "c0885dd3d48b4948b0ee81230ede97cc",
      "b8ca17dca5554f0a8b79e6d20c67a313",
      "870aa469408e459d9f9e32baae674f73",
      "25fa290bbc09475991a76faaecd2ce38",
      "07649b933d5949d8b9c32934224d8b8a",
      "0e43dd585370497a8b7df53cc62b275c",
      "674a6fe245584d4dbadc1160bfa150a3"
     ]
    },
    "executionInfo": {
     "elapsed": 3534,
     "status": "ok",
     "timestamp": 1755460694144,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "r3mLf8ixl7dD",
    "outputId": "860b3eb8-8981-483a-a1d3-e67a18b1c6b3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532d0d53e2da44c9b85d0e01cfd8c331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22473934ea94c69b5c608c3b4e89abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef221d18b95d4a38ad1b543c1b09f605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc55e92cae24888b56f627ef3023ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5995b41e9b1c4dd2bd68f02b996b726b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e492bf41c646348b37e0d8a7c3ed16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de609defe2ec43959a3bb3d82412cb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker ready âœ… CEPostprocessor\n"
     ]
    }
   ],
   "source": [
    "reranker = None\n",
    "try:\n",
    "    from llama_index.postprocessor import SentenceTransformerRerank\n",
    "    reranker = SentenceTransformerRerank(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3)\n",
    "except Exception:\n",
    "    try:\n",
    "        from llama_index.postprocessor.sentence_transformer_rerank import SentenceTransformerRerank\n",
    "        reranker = SentenceTransformerRerank(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3)\n",
    "    except Exception:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        class CEPostprocessor:\n",
    "            def __init__(self, model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=3):\n",
    "                self.ce = CrossEncoder(model); self.top_n = top_n\n",
    "            def postprocess_nodes(self, nodes, query_bundle):\n",
    "                q = getattr(query_bundle,\"query_str\",str(query_bundle))\n",
    "                txts=[]\n",
    "                for n in nodes:\n",
    "                    node = getattr(n,\"node\",n)\n",
    "                    txts.append(node.get_text() if hasattr(node,\"get_text\") else str(getattr(n,\"text\",\"\")))\n",
    "                scores = self.ce.predict([[q,t] for t in txts])\n",
    "                for sc,n in zip(scores,nodes):\n",
    "                    try: n.score=float(sc)\n",
    "                    except: pass\n",
    "                ranked=[n for _,n in sorted(zip(scores,nodes), key=lambda t:t[0], reverse=True)]\n",
    "                return ranked[:self.top_n]\n",
    "        reranker = CEPostprocessor(top_n=3)\n",
    "print(\"Reranker ready âœ…\", type(reranker).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gI2PpmhLmJxG"
   },
   "outputs": [],
   "source": [
    "import re, time, textwrap\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import Settings\n",
    "\n",
    "qe = RetrieverQueryEngine(retriever=hybrid_retriever, node_postprocessors=[reranker])\n",
    "\n",
    "def rewrite_query(q: str) -> str:\n",
    "    prompt = (\"Rewrite the user's question for precise retrieval in a lender fee worksheet. \"\n",
    "              \"Add key synonyms; keep it short.\\n\\n\"\n",
    "              f\"User: {q}\\nRewritten:\")\n",
    "    return Settings.llm.complete(prompt).text.strip()\n",
    "\n",
    "def ask_rag(q: str, expand=True, retries=3, base_sleep=1.2):\n",
    "    q2 = rewrite_query(q) if expand else q\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            resp = qe.query(q2)\n",
    "            print(f\"\\nQ: {q}\\nQ_expanded: {q2}\\n\")\n",
    "            print(\"Answer:\\n\", resp, \"\\n\")\n",
    "            print(\"Top sources:\")\n",
    "            for j, sn in enumerate(resp.source_nodes, 1):\n",
    "                node = getattr(sn,\"node\",sn)\n",
    "                name = getattr(node,\"metadata\",{}).get(\"file_name\",\"(unknown)\")\n",
    "                score = round((getattr(sn,\"score\",0) or 0), 3)\n",
    "                text = node.get_text() if hasattr(node,\"get_text\") else str(getattr(sn,\"text\",\"\"))\n",
    "                snippet = textwrap.shorten(text.replace(\"\\n\",\" \"), width=220)\n",
    "                print(f\"[{j}] score={score} file={name}\\n    {snippet}\")\n",
    "            return str(resp), resp.source_nodes\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) or \"TooManyRequests\" in str(e):\n",
    "                sleep = base_sleep*(2**i)\n",
    "                print(f\"Rate limit: retrying in {sleep:.1f}sâ€¦\")\n",
    "                time.sleep(sleep)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def extract_amount(sources, keywords):\n",
    "    \"\"\"Pick a $ amount near given keywords from top sources.\"\"\"\n",
    "    joined=\"\"\n",
    "    for sn in sources[:3]:\n",
    "        node = getattr(sn,\"node\",sn)\n",
    "        joined += \"\\n\" + (node.get_text() if hasattr(node,\"get_text\") else str(getattr(sn,\"text\",\"\")))\n",
    "    best=None\n",
    "    for line in joined.splitlines():\n",
    "        lo=line.lower()\n",
    "        if any(k in lo for k in keywords):\n",
    "            for m in re.findall(r\"\\$?\\s?\\d[\\d,]*(?:\\.\\d{2})?\", line):\n",
    "                best=m\n",
    "    if not best:\n",
    "        m=re.search(r\"\\$?\\s?\\d[\\d,]*(?:\\.\\d{2})?\", joined)\n",
    "        best=m.group(0) if m else None\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818
    },
    "executionInfo": {
     "elapsed": 3531,
     "status": "ok",
     "timestamp": 1755460710518,
     "user": {
      "displayName": "Mariam Agbila",
      "userId": "08167016603936367418"
     },
     "user_tz": 240
    },
    "id": "P3AI03WimKLf",
    "outputId": "288d0e1e-9e6f-43bb-a047-8c0b3eecb014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is the total estimated monthly payment?\n",
      "Q_expanded: Total monthly payment; Estimated payment; Monthly payment amount; Total payment due monthly\n",
      "\n",
      "Answer:\n",
      " The estimated monthly payment is $1,869.37.\n",
      " \n",
      "\n",
      "Top sources:\n",
      "[1] score=0.254 file=LenderFeesWorksheetNew (3).pdf\n",
      "    Your actual rate, payment, and cost could be higher. Get an official Loan Estimate before choosing a loan. Fee Details and Summary Applicants: Application No: Date Prepared: Loan Program: Prepared By: THIS IS NOT A [...]\n",
      "\n",
      "Estimated monthly payment (best evidence): 09\n",
      "\n",
      "Q: How much does the borrower pay for lender's title insurance?\n",
      "Q_expanded: Lender's title insurance cost; borrower's title insurance fee\n",
      "\n",
      "Answer:\n",
      " The lender's title insurance cost is not specified.  The borrower's title insurance fee is $650.00.\n",
      " \n",
      "\n",
      "Top sources:\n",
      "[1] score=-3.142 file=LenderFeesWorksheetNew (3).pdf\n",
      "    Your actual rate, payment, and cost could be higher. Get an official Loan Estimate before choosing a loan. Fee Details and Summary Applicants: Application No: Date Prepared: Loan Program: Prepared By: THIS IS NOT A [...]\n",
      "\n",
      "Lender's title insurance (best evidence): $ 475.00\n",
      "\n",
      "============================================================\n",
      "Short Explanation of Design Choices:\n",
      "- Embeddings: BAAI/bge-small-en-v1.5 â€” fast & strong for English retrieval.\n",
      "- Chunking: 1024 tokens, 200 overlap â€” balances recall with context.\n",
      "- Retrieval: Hybrid (BM25 + Vector, Î±=0.5) â€” exact fee names + semantic recall; cross-encoder reranker for precision.\n",
      "============================================================\n",
      "\n",
      "Response to Prompt 1:\n",
      " The estimated monthly payment is $1,869.37.\n",
      " \n",
      "Evidence amount: 09\n",
      "\n",
      "Response to Prompt 2:\n",
      " The lender's title insurance cost is not specified.  The borrower's title insurance fee is $650.00.\n",
      " \n",
      "Evidence amount: $ 475.00\n",
      "\n",
      "(If an amount is None, the PDF likely doesnâ€™t list it explicitlyâ€”your answer text + sources still count.)\n"
     ]
    }
   ],
   "source": [
    "# Prompt 1\n",
    "ans1, src1 = ask_rag(\"What is the total estimated monthly payment?\")\n",
    "amt1 = extract_amount(src1, keywords=(\"monthly\",\"payment\",\"estimated\",\"total\"))\n",
    "print(\"\\nEstimated monthly payment (best evidence):\", amt1)\n",
    "\n",
    "# Prompt 2\n",
    "ans2, src2 = ask_rag(\"How much does the borrower pay for lender's title insurance?\")\n",
    "amt2 = extract_amount(src2, keywords=(\"lender\",\"title\",\"insurance\",\"premium\"))\n",
    "print(\"\\nLender's title insurance (best evidence):\", amt2)\n",
    "\n",
    "# Final deliverables text (copy-paste into your submission form)\n",
    "choices = (\n",
    "  \"- Embeddings: BAAI/bge-small-en-v1.5 â€” fast & strong for English retrieval.\\n\"\n",
    "  \"- Chunking: 1024 tokens, 200 overlap â€” balances recall with context.\\n\"\n",
    "  \"- Retrieval: Hybrid (BM25 + Vector, Î±=0.5) â€” exact fee names + semantic recall; cross-encoder reranker for precision.\"\n",
    ")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Short Explanation of Design Choices:\\n\" + choices)\n",
    "print(\"=\"*60)\n",
    "print(\"\\nResponse to Prompt 1:\\n\", ans1, f\"\\nEvidence amount: {amt1}\")\n",
    "print(\"\\nResponse to Prompt 2:\\n\", ans2, f\"\\nEvidence amount: {amt2}\")\n",
    "print(\"\\n(If an amount is None, the PDF likely doesnâ€™t list it explicitlyâ€”your answer text + sources still count.)\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO5nehKEHYTIG6WANnkPdcI",
   "provenance": [
    {
     "file_id": "1dakixxCj54n_BsOH5DsBQuGC3UnpzhJj",
     "timestamp": 1755460933009
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
