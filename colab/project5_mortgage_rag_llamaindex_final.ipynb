{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSqzqZ0dNbByNGAEDLhtFM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma2070-spec/Outamation_AI_Externship/blob/main/colab/project5_mortgage_rag_llamaindex_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note for reviewers\n",
        "This notebook uses Colab User secrets or a masked prompt for `GOOGLE_API_KEY`.\n",
        "To run: File → Save a copy in Drive, then run cells top-to-bottom.\n",
        "The submitted notebook is view-only and outputs were cleared.\n"
      ],
      "metadata": {
        "id": "1_NK7-aSLvTf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPQz_ldnwet6"
      },
      "outputs": [],
      "source": [
        "# Install/refresh everything needed\n",
        "%pip -q install -U pip setuptools wheel\n",
        "%pip -q install -U \\\n",
        "  \"llama-index>=0.11\" \\\n",
        "  \"llama-index-llms-gemini>=0.2\" \\\n",
        "  \"llama-index-retrievers-bm25>=0.2\" \\\n",
        "  \"llama-index-embeddings-huggingface>=0.2\" \\\n",
        "  sentence-transformers pymupdf jedi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GOOGLE_API_KEY (Colab secrets -> fallback to masked prompt)\n",
        "import os\n",
        "def _load_secret(key_name=\"GOOGLE_API_KEY\"):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        v = userdata.get(key_name)\n",
        "        if v: return v\n",
        "    except Exception:\n",
        "        pass\n",
        "    v = os.environ.get(key_name)\n",
        "    if v: return v\n",
        "    from getpass import getpass\n",
        "    return getpass(f\"Enter your {key_name} (hidden): \")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = _load_secret(); del _load_secret\n",
        "\n",
        "# Imports (with compatibility fallbacks)\n",
        "try:\n",
        "    from llama_index.llms.gemini import Gemini\n",
        "except Exception as e:\n",
        "    raise ImportError(\"Gemini import failed. Did you run the install + restart?\") from e\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "try:\n",
        "    from llama_index.core.node_parser import SentenceSplitter\n",
        "except ImportError:\n",
        "    # Older/newer versions sometimes rename; this keeps things working\n",
        "    from llama_index.core.node_parser import TokenTextSplitter as SentenceSplitter\n",
        "\n",
        "print(\"Imports OK; key loaded:\", bool(os.environ.get(\"GOOGLE_API_KEY\")))\n",
        "\n"
      ],
      "metadata": {
        "id": "caf0vbTYw1eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: Configure dirs, knobs, and LlamaIndex settings ---\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Dirs\n",
        "DATA_DIR = Path(\"./data\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "STORAGE_DIR = Path(\"./storage\"); STORAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Retrieval knobs\n",
        "CHUNK_SIZE = 512\n",
        "CHUNK_OVERLAP = 32\n",
        "SIMILARITY_TOP_K = 5\n",
        "BM25_TOP_K = 5\n",
        "\n",
        "# Embeddings + rerank knobs\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "USE_RERANK = True\n",
        "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "RERANK_TOP_N = 5\n",
        "\n",
        "# LLM + embedding + chunker (handle model name differences)\n",
        "api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
        "try:\n",
        "    Settings.llm = Gemini(api_key=api_key, model=\"models/gemini-1.5-flash\")\n",
        "except Exception:\n",
        "    # Some versions expect the short name\n",
        "    Settings.llm = Gemini(api_key=api_key, model=\"gemini-1.5-flash\")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\n",
        "print(\"Configured ✅\")\n",
        "print(\"Data dir:\", DATA_DIR.resolve())\n"
      ],
      "metadata": {
        "id": "4lf7uN_R3c4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core imports\n",
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "# LlamaIndex pieces\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex, get_response_synthesizer\n",
        "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
        "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
        "\n",
        "# Splitter name sometimes differs across versions\n",
        "try:\n",
        "    from llama_index.core.node_parser import SentenceSplitter\n",
        "except ImportError:\n",
        "    from llama_index.core.node_parser import TokenTextSplitter as SentenceSplitter\n",
        "\n",
        "# Reranker import\n",
        "try:\n",
        "    from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "except ImportError:\n",
        "    SentenceTransformerRerank = None  # we'll handle None below\n",
        "\n",
        "# LLM + embeddings + BM25\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "print(\"Imports OK ✅\")\n"
      ],
      "metadata": {
        "id": "aOqGP0AK3iPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dirs\n",
        "DATA_DIR = Path(\"./data\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "STORAGE_DIR = Path(\"./storage\"); STORAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Retrieval knobs\n",
        "CHUNK_SIZE = 512\n",
        "CHUNK_OVERLAP = 32\n",
        "SIMILARITY_TOP_K = 5\n",
        "BM25_TOP_K = 5\n",
        "\n",
        "# Embeddings + rerank knobs\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "USE_RERANK = True\n",
        "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "RERANK_TOP_N = 5\n",
        "\n",
        "# LLM + embedding + chunker\n",
        "api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
        "try:\n",
        "    Settings.llm = Gemini(api_key=api_key, model=\"models/gemini-1.5-flash\")\n",
        "except Exception:\n",
        "    # some versions expect short model name\n",
        "    Settings.llm = Gemini(api_key=api_key, model=\"gemini-1.5-flash\")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\n",
        "print(\"Configured ✅  Data dir:\", DATA_DIR.resolve())\n"
      ],
      "metadata": {
        "id": "69hWmNBa4TR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you're in Colab, this opens a file picker\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "    uploaded = files.upload()\n",
        "    for name, content in uploaded.items():\n",
        "        with open(DATA_DIR / name, \"wb\") as f:\n",
        "            f.write(content)\n",
        "    print(\"Saved:\", list(DATA_DIR.glob(\"*\")))\n",
        "except Exception as e:\n",
        "    print(\"If not in Colab, just place PDFs into ./data manually.\")\n"
      ],
      "metadata": {
        "id": "CW50s7a_4cpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 7 (updated): Build vector index + BM25 using same chunks ---\n",
        "\n",
        "# 1) Load docs\n",
        "docs = SimpleDirectoryReader(str(DATA_DIR)).load_data()\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "\n",
        "# 2) Vector index (dense)\n",
        "vector_index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
        "\n",
        "# 3) Create nodes for BM25 using the same splitter config\n",
        "parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\n",
        "nodes = None\n",
        "try:\n",
        "    # Newer API\n",
        "    nodes = parser.get_nodes_from_documents(docs)\n",
        "except Exception:\n",
        "    # Fallback: older API name (some builds)\n",
        "    try:\n",
        "        nodes = parser.build_nodes_from_documents(docs)  # fallback name\n",
        "    except Exception as e:\n",
        "        print(\"Couldn't create nodes via parser; will pass documents directly to BM25.\\n\", e)\n",
        "\n",
        "# 4) BM25 retriever (prefer nodes; otherwise documents)\n",
        "if nodes:\n",
        "    bm25 = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=BM25_TOP_K)\n",
        "else:\n",
        "    bm25 = BM25Retriever.from_defaults(documents=docs, similarity_top_k=BM25_TOP_K)\n",
        "\n",
        "# 5) Dense retriever\n",
        "# (Using as_retriever is more version-proof than VectorIndexRetriever)\n",
        "vector_retriever = vector_index.as_retriever(similarity_top_k=SIMILARITY_TOP_K)\n",
        "\n",
        "print(\"Indexes ready: vector (dense) + BM25 (sparse)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8E-b1kS9418e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridRRFRetriever(BaseRetriever):\n",
        "    def __init__(self, retrievers, top_k=5, rrf_k=60):\n",
        "        self.retrievers = retrievers\n",
        "        self.top_k = top_k\n",
        "        self.rrf_k = rrf_k\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        scores = {}\n",
        "        node_map = {}\n",
        "        for retr in self.retrievers:\n",
        "            results = retr.retrieve(query_bundle.query_str)\n",
        "            for rank, res in enumerate(results, 1):\n",
        "                nid = res.node.node_id\n",
        "                node_map[nid] = res.node\n",
        "                scores[nid] = scores.get(nid, 0.0) + 1.0 / (self.rrf_k + rank)\n",
        "        fused = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:self.top_k]\n",
        "        return [NodeWithScore(node=node_map[nid], score=sc) for nid, sc in fused]\n",
        "\n",
        "hybrid_retriever = HybridRRFRetriever(\n",
        "    retrievers=[bm25, vector_retriever],\n",
        "    top_k=max(SIMILARITY_TOP_K, BM25_TOP_K)\n",
        ")\n",
        "print(\"Hybrid retriever ready.\")\n"
      ],
      "metadata": {
        "id": "Zy6Paaex5iR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reranker = None\n",
        "if USE_RERANK and SentenceTransformerRerank is not None:\n",
        "    reranker = SentenceTransformerRerank(model=RERANK_MODEL, top_n=RERANK_TOP_N)\n",
        "    print(\"Reranker loaded:\", RERANK_MODEL)\n",
        "else:\n",
        "    print(\"Reranker disabled or package unavailable.\")\n"
      ],
      "metadata": {
        "id": "QsB3qGhZ5oc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
        "\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=hybrid_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    node_postprocessors=[reranker] if reranker else None,\n",
        ")\n",
        "print(\"Query engine ready.\")\n"
      ],
      "metadata": {
        "id": "Iw8IsHPm5vWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(q: str, show_sources: bool = True):\n",
        "    print(f\"Q: {q}\\n\")\n",
        "    resp = query_engine.query(q)\n",
        "    print(\"A:\", str(resp), \"\\n\")\n",
        "    if show_sources and getattr(resp, \"source_nodes\", None):\n",
        "        rows = []\n",
        "        for i, sn in enumerate(resp.source_nodes, 1):\n",
        "            m = sn.node.metadata or {}\n",
        "            rows.append({\n",
        "                \"rank\": i,\n",
        "                \"score\": round(getattr(sn, \"score\", 0.0), 4),\n",
        "                \"file\": m.get(\"file_name\", m.get(\"filename\", \"\")),\n",
        "                \"page\": m.get(\"page_label\", m.get(\"page\", \"\")),\n",
        "                \"preview\": (sn.node.get_content() or \"\")[:240].replace(\"\\n\", \" \")\n",
        "            })\n",
        "        if rows:\n",
        "            display(pd.DataFrame(rows))\n",
        "    return resp\n",
        "\n",
        "# Try:\n",
        "# ask(\"What is the borrower's base interest rate and any discount points? Cite the page.\")\n",
        "# ask(\"Explain escrow requirements and any conditions for waivers with citations.\")\n"
      ],
      "metadata": {
        "id": "brVUAFZp5yim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_questions = [\n",
        "    \"Summarize borrower obligations for taxes and insurance with citations.\",\n",
        "    \"How is private mortgage insurance handled? Include any thresholds and cite.\",\n",
        "    \"What late fee policy applies and on which page is it described?\"\n",
        "]\n",
        "\n",
        "def run_eval(questions):\n",
        "    recs = []\n",
        "    for q in questions:\n",
        "        resp = query_engine.query(q)\n",
        "        cites = []\n",
        "        for sn in resp.source_nodes:\n",
        "            m = sn.node.metadata or {}\n",
        "            cites.append(f\"{m.get('file_name','')} p.{m.get('page_label', m.get('page',''))}\")\n",
        "        recs.append({\n",
        "            \"question\": q,\n",
        "            \"answer_len\": len(str(resp)),\n",
        "            \"num_citations\": len(cites),\n",
        "            \"citations\": \"; \".join([c for c in cites if c])\n",
        "        })\n",
        "    return pd.DataFrame(recs)\n",
        "\n",
        "# run_eval(eval_questions)\n"
      ],
      "metadata": {
        "id": "2wYbgdjj52z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index.storage_context.persist(persist_dir=str(STORAGE_DIR))\n",
        "print(\"Persisted vector index to\", STORAGE_DIR.resolve())\n"
      ],
      "metadata": {
        "id": "njQ4qlHG589X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"what is the borrower's base interest rate and nay discount points? Cite the page\")"
      ],
      "metadata": {
        "id": "22uL8_pY6QZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"Explain escrow requirements and any conditions for waivers with citations.\")"
      ],
      "metadata": {
        "id": "RqOkg0GJ64cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"List all fees affecting APR and where they appear in the document.\")"
      ],
      "metadata": {
        "id": "wsTfnGw869Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"What late fee policy applies and on which page is it described?\")"
      ],
      "metadata": {
        "id": "5sWMXQqA7GAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = run_eval(eval_questions)\n",
        "df\n"
      ],
      "metadata": {
        "id": "yqzklnwa7WKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index.storage_context.persist(persist_dir=str(STORAGE_DIR))\n",
        "print(\"Persisted to\", STORAGE_DIR.resolve())\n"
      ],
      "metadata": {
        "id": "_dMGWQfP7gS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 5 – Mortgage RAG Pipeline (LlamaIndex)\n",
        "\n",
        "**Goal:** Build a working Retrieval-Augmented Generation (RAG) assistant for mortgage PDFs that returns grounded answers with citations.\n",
        "\n",
        "## What this notebook does\n",
        "- Ingests mortgage PDFs in `./data`\n",
        "- Chunks text and builds hybrid retrieval (BM25 + dense embeddings)\n",
        "- (Optional) re-ranks top candidates\n",
        "- Answers questions with a chosen LLM (Gemini or Mistral) and shows citations\n",
        "\n",
        "## How to run\n",
        "1) Install dependencies (Step 1) and then Restart session (Step 2).  \n",
        "2) Load your GOOGLE_API_KEY (Step 3).  \n",
        "3) Run imports and config (Steps 4–5).  \n",
        "4) Upload 1+ mortgage PDFs into `./data` (Step 6).  \n",
        "5) Build the vector index and BM25 (Step 7).  \n",
        "6) Create the hybrid retriever and (optional) reranker (Steps 8–9).  \n",
        "7) Use `ask(\"your question\")` to query and see citations (Steps 10–11).  \n",
        "8) Run a quick evaluation table (Step 12).  \n",
        "9) (Optional) Persist the index (Step 13).\n",
        "\n",
        "## Settings we tuned\n",
        "- Chunk size: 512, overlap: 32  \n",
        "- similarity_top_k: 5, BM25 top_k: 5  \n",
        "- Hybrid retrieval via RRF, optional cross-encoder re-rank\n",
        "\n",
        "## Security\n",
        "- API key is loaded via Colab secrets or a masked prompt (not printed).  \n",
        "- Use sanitized/sample documents if originals are sensitive.\n",
        "\n",
        "## Links to earlier projects (optional)\n",
        "- P1: Text PDFs + regex extraction  \n",
        "- P2: Preprocessing (binarize, deskew, denoise)  \n",
        "- P3: OCR comparison (Tesseract vs PaddleOCR)  \n",
        "- P4: First RAG build with chunking/embeddings/retrieval tuning\n",
        "\n",
        "## Submission checklist\n",
        "- Share this Colab as Viewer and paste the link into the Project 5 form (#1).  \n",
        "- Create a short Google Doc reflection (about 100 words) and paste the link into the form (#2).  \n",
        "- Paste your comparison insights (150–200 words) into the form (#3).  \n",
        "- Build a brief presentation summarizing pipeline, tuning, example Q->A with citations, and lessons learned.\n",
        "  "
      ],
      "metadata": {
        "id": "TTQvDxeqWnjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection (about 100 words)\n",
        "Across Projects 1-4 I built a mortgage document pipeline from raw PDFs to a working RAG assistant. I started with text parsing and regex field extraction, then handled scanned files with OCR (Tesseract and PaddleOCR) and preprocessing to improve legibility. Next, I indexed documents in LlamaIndex, added chunking and Hugging Face embeddings, and tuned retrieval (top_k, overlap, hybrid BM25 plus dense). Finally, I connected an LLM (Gemini or Mistral) and enforced source-grounded answers. The result is a reproducible notebook that ingests PDFs, retrieves the right spans, and cites them. I strengthened Python, PDF and OCR tooling, evaluation, and prompt and retrieval design.\n",
        "\n",
        "## Comparison Insights (150–200 words)\n",
        "I compared several retrieval setups and two LLMs to see what most improved grounded answers. Dense-only retrieval with large 1,000-token chunks often returned broad context and produced vague summaries. Reducing chunks to about 400–600 tokens with 20–50 overlap increased the chance that a single chunk contained the full rate or fee detail needed for precise citations. Adding a hybrid retriever (BM25 plus dense) consistently surfaced term-specific pages (for example, escrow waivers or FHA MIP) that dense-only sometimes missed. A lightweight cross-encoder re-ranker on the top 10 candidates further improved multi-part questions by pushing the most relevant spans to the top. Gemini tended to produce more structured, citation-friendly answers out of the box, while Mistral responded faster and was cheaper but needed stricter prompts to reference sources consistently. Overall, retrieval quality (chunking, hybrid retrieval, and re-ranking) mattered more than the model choice; once context was strong, both LLMs performed well. My final settings use ~512-token chunks, overlap 32, similarity_top_k 5, hybrid on, and citation-first prompts.\n"
      ],
      "metadata": {
        "id": "bDHkaW-kYPh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Troubleshooting & Tips\n",
        "\n",
        "## Quick diagnostics\n",
        "- After installs, always do **Runtime → Restart session**.\n",
        "- Verify key & imports:\n",
        "  ```python\n",
        "  import os\n",
        "  print(\"GOOGLE_API_KEY set:\", bool(os.environ.get(\"GOOGLE_API_KEY\")))\n",
        "  from llama_index.llms.gemini import Gemini\n",
        "  from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n"
      ],
      "metadata": {
        "id": "kI178VDxYo47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U \"jedi>=0.18.2\"\n"
      ],
      "metadata": {
        "id": "P0Giy8FYeLnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U pip setuptools wheel\n",
        "%pip -q install -U \"llama-index>=0.11\" \"llama-index-llms-gemini>=0.2\" \\\n",
        "  \"llama-index-retrievers-bm25>=0.2\" \"llama-index-embeddings-huggingface>=0.2\" \\\n",
        "  sentence-transformers pymupdf jedi\n"
      ],
      "metadata": {
        "id": "cTWOqCbbZ86m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}